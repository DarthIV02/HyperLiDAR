{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9acb6a35-636a-40f0-88c4-fe96d208ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.waffleiron.segmenter import Segmenter\n",
    "import torch\n",
    "from datasets import LIST_DATASETS, Collate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torchhd\n",
    "from torchhd.models import Centroid\n",
    "from torchhd import embeddings\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c56493a-9864-4629-b228-487caaf00ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor:\n",
    "    def __init__(self, input_channels=5, feat_channels=768, depth=48, \n",
    "                 grid_shape=[[256, 256], [256, 32], [256, 32]], nb_class=16, layer_norm=True, \n",
    "                 device=torch.device(\"cpu\"), early_exit = 48, **kwargs):\n",
    "        self.model = Segmenter(\n",
    "            input_channels=input_channels,\n",
    "            feat_channels=feat_channels,\n",
    "            depth=depth,\n",
    "            grid_shape=grid_shape,\n",
    "            nb_class=nb_class, # class for prediction\n",
    "            #drop_path_prob=config[\"waffleiron\"][\"drop_path\"],\n",
    "            layer_norm=layer_norm,\n",
    "        )\n",
    "\n",
    "        classif = torch.nn.Conv1d(\n",
    "            feat_channels, nb_class, 1 # So it fits 16 = nb_class but classifier is not used\n",
    "        )\n",
    "        torch.nn.init.constant_(classif.bias, 0)\n",
    "        torch.nn.init.constant_(classif.weight, 0)\n",
    "        self.model.classif = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(feat_channels),\n",
    "            classif,\n",
    "        )\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.model.classif.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        def get_optimizer(parameters):\n",
    "            return torch.optim.AdamW(\n",
    "                parameters,\n",
    "                lr=0.001,\n",
    "                weight_decay=0.003,\n",
    "            )\n",
    "\n",
    "        optim = get_optimizer(self.model.parameters())\n",
    "        self.device = device\n",
    "        self.device_string = \"cuda:0\"\n",
    "        self.num_classes = nb_class\n",
    "        self.early_exit = early_exit\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def load_pretrained(self, path):\n",
    "        # Load pretrained model\n",
    "        path_to_ckpt = path\n",
    "        checkpoint = torch.load(path_to_ckpt,\n",
    "            map_location=self.device_string)\n",
    "        state_dict = checkpoint[\"net\"]  # Adjust key as needed\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            new_key = k.replace(\"module.\", \"\")  # Remove \"module.\" prefix\n",
    "            new_state_dict[new_key] = v\n",
    "\n",
    "        self.model.load_state_dict(new_state_dict)\n",
    "\n",
    "        print(\n",
    "            f\"Checkpoint loaded on {self.device_string}: {path_to_ckpt}\"\n",
    "        )\n",
    "\n",
    "        if self.device_string != 'cpu':\n",
    "            torch.cuda.set_device(self.device_string) # cuda:0\n",
    "            self.model = self.model.cuda(self.device_string) # cuda:0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward_model(self, it, batch):\n",
    "        pts_og = batch[\"points\"]\n",
    "        labels_og = batch[\"labels\"]\n",
    "        pts_voxel = batch[\"points_voxel\"]\n",
    "        feat = batch[\"feat\"]\n",
    "        labels = batch[\"labels_orig\"]\n",
    "        cell_ind = batch[\"cell_ind\"]\n",
    "        occupied_cell = batch[\"occupied_cells\"]\n",
    "        neighbors_emb = batch[\"neighbors_emb\"]\n",
    "        if self.device_string != 'cpu':\n",
    "            feat = feat.cuda(0, non_blocking=True)\n",
    "            labels = labels.cuda(0, non_blocking=True)\n",
    "            batch[\"upsample\"] = [\n",
    "                up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "            ]\n",
    "            cell_ind = cell_ind.cuda(0, non_blocking=True)\n",
    "            occupied_cell = occupied_cell.cuda(0, non_blocking=True)\n",
    "            neighbors_emb = neighbors_emb.cuda(0, non_blocking=True)\n",
    "        net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "        if self.device_string != 'cpu':\n",
    "            with torch.autocast(\"cuda\", enabled=True):\n",
    "                # Logits\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(*net_inputs, self.early_exit)\n",
    "                    encode, tokens, out = out[0], out[1], out[2]\n",
    "                    pred_label = out.max(1)[1]\n",
    "\n",
    "                    # Only return samples that are not noise\n",
    "                    #torch.cuda.synchronize(device=self.device)\n",
    "                    where = labels != 255\n",
    "                    #torch.cuda.synchronize(device=self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.model(*net_inputs, self.early_exit)\n",
    "                encode, tokens, out = out[0], out[1], out[2]\n",
    "                pred_label = out.max(1)[1]\n",
    "\n",
    "                # Only return samples that are not noise\n",
    "                where = labels != 255\n",
    "\n",
    "        pts_og = torch.tensor(pts_og[0]).to(self.device)\n",
    "        pts_voxel = torch.tensor(pts_voxel[0]).to(self.device)\n",
    "        where = where.cpu()\n",
    "        return pts_og, labels_og, pts_voxel[where], tokens[0,:,where], labels[where], pred_label[0, where]\n",
    "\n",
    "    def test(self, loader, stop):        \n",
    "        # Metric\n",
    "        miou = MulticlassJaccardIndex(num_classes=self.num_classes, average=None).to(self.device, non_blocking=True)\n",
    "        final_labels_sep = torch.empty((1, 90000), device=self.device)\n",
    "        final_labels_vox_sep = torch.empty((1, 90000), device=self.device)\n",
    "        final_feat_sep = torch.empty((1, 768, 90000), device=self.device)\n",
    "        final_soa_result_sep = torch.empty((1, 90000), device=self.device)\n",
    "        final_points_og_sep = torch.empty((1, 3, 90000), device=self.device)\n",
    "        final_points_sep = torch.empty((1, 3, 90000), device=self.device)\n",
    "        num_voxels = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        for it, batch in tqdm(enumerate(loader), desc=\"SoA testing\"):\n",
    "            pts_og, labels_og, pts, features, labels, soa_result = self.forward_model(it, batch)\n",
    "\n",
    "            # Print og labels and points\n",
    "            pts_og = torch.transpose(pts_og, 0, 1)\n",
    "            points_here = torch.cat((pts_og[:3, :], torch.reshape(torch.tensor((labels_og)), (1, pts_og.shape[1]))), dim=0)\n",
    "            points_here = torch.transpose(points_here, 0, 1)\n",
    "            points_here[points_here[:, 3] == 255, 3] = -1 # SemanticKitti\n",
    "            print(points_here.shape)\n",
    "            \n",
    "            # Convert to NumPy\n",
    "            tensor_np = points_here.cpu().numpy()\n",
    "            \n",
    "            # Save as CSV\n",
    "            df = pd.DataFrame(tensor_np)\n",
    "            df.to_csv(f\"tensor_semkitti_{it}_og.csv\", index=False, header=False)\n",
    "            shape_sample = labels.shape[0]\n",
    "            num_voxels.append(shape_sample)\n",
    "            labels_tensor = torch.reshape(torch.Tensor(labels), (1,shape_sample)).to(self.device)\n",
    "            feat_tensor = torch.reshape(torch.Tensor(features), (1,768,shape_sample)).to(self.device)\n",
    "            soa_tensor = torch.reshape(torch.Tensor(soa_result), (1,shape_sample)).to(self.device)\n",
    "            pts_tensor = torch.reshape(torch.Tensor(pts), (1,3,shape_sample)).to(self.device)\n",
    "            final_labels_sep = torch.concat((final_labels_sep, F.pad(input=labels_tensor, pad=(0, 90000 - shape_sample), mode='constant', value=0)))\n",
    "            final_soa_result_sep = torch.concat((final_soa_result_sep, F.pad(input=soa_tensor, pad=(0, 90000 - shape_sample), mode='constant', value=0)))\n",
    "            final_feat_sep = torch.concat((final_feat_sep, F.pad(input=feat_tensor, pad=(0, 90000 - shape_sample), mode='constant', value=0)))\n",
    "            final_points_sep = torch.concat((final_points_sep, F.pad(input=pts_tensor, pad=(0, 90000 - shape_sample), mode='constant', value=0)))\n",
    "            print(final_labels_sep.shape)\n",
    "            print(final_soa_result_sep.shape)\n",
    "            print(final_feat_sep.shape)\n",
    "            print(final_points_sep.shape)\n",
    "\n",
    "            #labels = labels.to(dtype = torch.int64, device = self.device, non_blocking=True)\n",
    "            #soa_result = soa_result.to(device=self.device, non_blocking=True)\n",
    "            #final_labels[start_idx:start_idx+shape_sample] = labels\n",
    "\n",
    "            #final_pred[start_idx:start_idx+shape_sample] = soa_result\n",
    "\n",
    "            #start_idx += shape_sample\n",
    "\n",
    "            if it == stop:\n",
    "                break\n",
    "\n",
    "        #final_labels = final_labels[:start_idx]\n",
    "        #final_pred = final_pred[:start_idx]\n",
    "\n",
    "        #print(\"================================\")\n",
    "\n",
    "        #print('Pred FE', final_pred, \"\\tShape: \", final_pred.shape)\n",
    "        #print('Label', final_labels, \"\\tShape: \", final_labels.shape)\n",
    "        #accuracy = miou(final_pred, final_labels)\n",
    "        #avg_acc = torch.mean(accuracy)\n",
    "        #print(f'accuracy: {accuracy}')\n",
    "        #print(f'avg acc: {avg_acc}')\n",
    "\n",
    "        #cm = confusion_matrix(pred_hd, first_label, labels=torch.Tensor(range(0,15)))\n",
    "        #print(\"Confusion matrix \\n\")\n",
    "        #print(cm)\n",
    "\n",
    "        #print(\"================================\")\n",
    "\n",
    "        return final_points_sep, final_labels_sep, final_soa_result_sep, final_feat_sep, num_voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d153176-3499-4237-bcea-9f5b7b74f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183589/1083699130.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path_to_ckpt,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded on cuda:0: /root/main/ScaLR/saved_models/ckpt_last_kitti.pth\n"
     ]
    }
   ],
   "source": [
    "fe = Feature_Extractor(nb_class=19) # 19\n",
    "fe.load_pretrained('/root/main/ScaLR/saved_models/ckpt_last_kitti.pth') #  ckpt_last_scalr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb18eb4-68ba-4b21-aa59-77bd06f74f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 1.011 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "kwargs = { # Nuscenes and semantickitti\n",
    "    \"rootdir\": '/root/main/dataset/semantickitti', # nuscenes\n",
    "    \"input_feat\": [\"intensity\", \"xyz\", \"radius\"],\n",
    "    \"voxel_size\": 0.1,\n",
    "    \"num_neighbors\": 16,\n",
    "    \"dim_proj\": [2, 1, 0],\n",
    "    \"grids_shape\": [[256, 256], [256, 32], [256, 32]],\n",
    "    \"fov_xyz\": [[-64, -64, -8], [64, 64, 8]], # Check here\n",
    "}\n",
    "\n",
    "### FALSE NOT THE TRUE VALUES\n",
    "\n",
    "#kwargs = {\n",
    "#    \"rootdir\": '/root/main/dataset/semantickitti',\n",
    "#    \"input_feat\": [\"intensity\", \"xyz\", \"radius\"],\n",
    "#    \"voxel_size\": 0.1,\n",
    "#    \"num_neighbors\": 16,\n",
    "#    \"dim_proj\": [2, 1, 0],\n",
    "#    \"grids_shape\": [[250, 250], [250, 12], [250, 12]],\n",
    "#    \"fov_xyz\": [[-50, -50, -3], [50, 50, 2]], # Check here\n",
    "#}\n",
    "\n",
    "# Get datatset\n",
    "DATASET = LIST_DATASETS.get('semantic_kitti') #  nuscenes\n",
    "\n",
    "#### SemanticKitti ####\n",
    "\"\"\"\n",
    "dataset_train = DATASET(\n",
    "    phase=\"train\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = DATASET(\n",
    "    phase=\"val\",\n",
    "    **kwargs,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "##### Nuscenes ####\n",
    "# Train dataset\n",
    "dataset = DATASET(\n",
    "    phase=\"train\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "dataset_train = copy.deepcopy(dataset)\n",
    "dataset_val = copy.deepcopy(dataset)\n",
    "dataset_train.init_training()\n",
    "dataset_val.init_val()\n",
    "\n",
    "num_classes = 16 # 19\n",
    "\n",
    "path_pretrained = '/root/main/ScaLR/saved_models/ckpt_last_kitti.pth' # ckpt_last_scalr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e79b6c1-4d7b-4a94-bb28-45ff709105c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=Collate(device=device),\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=Collate(device=device),\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac5c66e4-f4b2-4b37-998a-b7396055fef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34688, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 1it [00:02,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 90000])\n",
      "torch.Size([2, 90000])\n",
      "torch.Size([2, 768, 90000])\n",
      "torch.Size([2, 3, 90000])\n",
      "torch.Size([34688, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 2it [00:04,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 90000])\n",
      "torch.Size([3, 90000])\n",
      "torch.Size([3, 768, 90000])\n",
      "torch.Size([3, 3, 90000])\n",
      "torch.Size([34720, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 3it [00:07,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 90000])\n",
      "torch.Size([4, 90000])\n",
      "torch.Size([4, 768, 90000])\n",
      "torch.Size([4, 3, 90000])\n",
      "torch.Size([34720, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 4it [00:10,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 90000])\n",
      "torch.Size([5, 90000])\n",
      "torch.Size([5, 768, 90000])\n",
      "torch.Size([5, 3, 90000])\n",
      "torch.Size([34720, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 5it [00:14,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 90000])\n",
      "torch.Size([6, 90000])\n",
      "torch.Size([6, 768, 90000])\n",
      "torch.Size([6, 3, 90000])\n",
      "torch.Size([34720, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 6it [00:17,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 90000])\n",
      "torch.Size([7, 90000])\n",
      "torch.Size([7, 768, 90000])\n",
      "torch.Size([7, 3, 90000])\n",
      "torch.Size([34688, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 7it [00:20,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 90000])\n",
      "torch.Size([8, 90000])\n",
      "torch.Size([8, 768, 90000])\n",
      "torch.Size([8, 3, 90000])\n",
      "torch.Size([34720, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 8it [00:24,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 90000])\n",
      "torch.Size([9, 90000])\n",
      "torch.Size([9, 768, 90000])\n",
      "torch.Size([9, 3, 90000])\n",
      "torch.Size([34752, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 9it [00:27,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 90000])\n",
      "torch.Size([10, 90000])\n",
      "torch.Size([10, 768, 90000])\n",
      "torch.Size([10, 3, 90000])\n",
      "torch.Size([34720, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 10it [00:31,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 90000])\n",
      "torch.Size([11, 90000])\n",
      "torch.Size([11, 768, 90000])\n",
      "torch.Size([11, 3, 90000])\n",
      "torch.Size([34688, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 10it [00:35,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 90000])\n",
      "torch.Size([12, 90000])\n",
      "torch.Size([12, 768, 90000])\n",
      "torch.Size([12, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pts_voxel, labels_train, soa_train, feat_train, vox_train = fe.test(train_loader, 10) # With the nuscenes hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab55da7f-7081-48b3-9a77-d43fcea520e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16247, 12222, 14237, 17581, 17113, 14250, 15004, 16066, 16474, 14231,\n",
       "        16595])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(vox_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebfe3a5b-21d7-4067-9f24-c0bead7c0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labels_train[1:], 'labels_train_nuscenes.pt')\n",
    "torch.save(soa_train[1:], 'soa_train_nuscenes.pt')\n",
    "torch.save(feat_train[1:], 'feat_train_nuscenes.pt')\n",
    "torch.save(pts_voxel[1:], 'pts_train_nuscenes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d03f98-c1c1-4d38-9caf-a8efd5f7edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(vox_train), 'voxels_train_nuscenes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e102fd65-f15f-459c-b161-b410ebd07d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 1it [00:03,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 90000])\n",
      "torch.Size([2, 90000])\n",
      "torch.Size([2, 768, 90000])\n",
      "torch.Size([2, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 2it [00:07,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 90000])\n",
      "torch.Size([3, 90000])\n",
      "torch.Size([3, 768, 90000])\n",
      "torch.Size([3, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 3it [00:11,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 90000])\n",
      "torch.Size([4, 90000])\n",
      "torch.Size([4, 768, 90000])\n",
      "torch.Size([4, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 4it [00:16,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 90000])\n",
      "torch.Size([5, 90000])\n",
      "torch.Size([5, 768, 90000])\n",
      "torch.Size([5, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 5it [00:22,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 90000])\n",
      "torch.Size([6, 90000])\n",
      "torch.Size([6, 768, 90000])\n",
      "torch.Size([6, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 6it [00:26,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 90000])\n",
      "torch.Size([7, 90000])\n",
      "torch.Size([7, 768, 90000])\n",
      "torch.Size([7, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 7it [00:30,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 90000])\n",
      "torch.Size([8, 90000])\n",
      "torch.Size([8, 768, 90000])\n",
      "torch.Size([8, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 8it [00:35,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 90000])\n",
      "torch.Size([9, 90000])\n",
      "torch.Size([9, 768, 90000])\n",
      "torch.Size([9, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 9it [00:39,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 90000])\n",
      "torch.Size([10, 90000])\n",
      "torch.Size([10, 768, 90000])\n",
      "torch.Size([10, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 10it [00:45,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 90000])\n",
      "torch.Size([11, 90000])\n",
      "torch.Size([11, 768, 90000])\n",
      "torch.Size([11, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 11it [00:50,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 90000])\n",
      "torch.Size([12, 90000])\n",
      "torch.Size([12, 768, 90000])\n",
      "torch.Size([12, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 12it [00:55,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 90000])\n",
      "torch.Size([13, 90000])\n",
      "torch.Size([13, 768, 90000])\n",
      "torch.Size([13, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 13it [01:02,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 90000])\n",
      "torch.Size([14, 90000])\n",
      "torch.Size([14, 768, 90000])\n",
      "torch.Size([14, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 14it [01:08,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 90000])\n",
      "torch.Size([15, 90000])\n",
      "torch.Size([15, 768, 90000])\n",
      "torch.Size([15, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 15it [01:15,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 90000])\n",
      "torch.Size([16, 90000])\n",
      "torch.Size([16, 768, 90000])\n",
      "torch.Size([16, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 16it [01:22,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 90000])\n",
      "torch.Size([17, 90000])\n",
      "torch.Size([17, 768, 90000])\n",
      "torch.Size([17, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 17it [01:28,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 90000])\n",
      "torch.Size([18, 90000])\n",
      "torch.Size([18, 768, 90000])\n",
      "torch.Size([18, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 18it [01:35,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 90000])\n",
      "torch.Size([19, 90000])\n",
      "torch.Size([19, 768, 90000])\n",
      "torch.Size([19, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 19it [01:41,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 90000])\n",
      "torch.Size([20, 90000])\n",
      "torch.Size([20, 768, 90000])\n",
      "torch.Size([20, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 20it [01:49,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 90000])\n",
      "torch.Size([21, 90000])\n",
      "torch.Size([21, 768, 90000])\n",
      "torch.Size([21, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 21it [01:55,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 90000])\n",
      "torch.Size([22, 90000])\n",
      "torch.Size([22, 768, 90000])\n",
      "torch.Size([22, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 22it [02:02,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 90000])\n",
      "torch.Size([23, 90000])\n",
      "torch.Size([23, 768, 90000])\n",
      "torch.Size([23, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 23it [02:09,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 90000])\n",
      "torch.Size([24, 90000])\n",
      "torch.Size([24, 768, 90000])\n",
      "torch.Size([24, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 24it [02:16,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 90000])\n",
      "torch.Size([25, 90000])\n",
      "torch.Size([25, 768, 90000])\n",
      "torch.Size([25, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 25it [02:26,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 90000])\n",
      "torch.Size([26, 90000])\n",
      "torch.Size([26, 768, 90000])\n",
      "torch.Size([26, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 26it [02:35,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 90000])\n",
      "torch.Size([27, 90000])\n",
      "torch.Size([27, 768, 90000])\n",
      "torch.Size([27, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 27it [02:42,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 90000])\n",
      "torch.Size([28, 90000])\n",
      "torch.Size([28, 768, 90000])\n",
      "torch.Size([28, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 28it [02:52,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 90000])\n",
      "torch.Size([29, 90000])\n",
      "torch.Size([29, 768, 90000])\n",
      "torch.Size([29, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 29it [03:02,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 90000])\n",
      "torch.Size([30, 90000])\n",
      "torch.Size([30, 768, 90000])\n",
      "torch.Size([30, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 30it [03:10,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 90000])\n",
      "torch.Size([31, 90000])\n",
      "torch.Size([31, 768, 90000])\n",
      "torch.Size([31, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 31it [03:21,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 90000])\n",
      "torch.Size([32, 90000])\n",
      "torch.Size([32, 768, 90000])\n",
      "torch.Size([32, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 32it [03:30,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 90000])\n",
      "torch.Size([33, 90000])\n",
      "torch.Size([33, 768, 90000])\n",
      "torch.Size([33, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 33it [03:39,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 90000])\n",
      "torch.Size([34, 90000])\n",
      "torch.Size([34, 768, 90000])\n",
      "torch.Size([34, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 34it [03:47,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 90000])\n",
      "torch.Size([35, 90000])\n",
      "torch.Size([35, 768, 90000])\n",
      "torch.Size([35, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 35it [03:57,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 90000])\n",
      "torch.Size([36, 90000])\n",
      "torch.Size([36, 768, 90000])\n",
      "torch.Size([36, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 36it [04:05,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 90000])\n",
      "torch.Size([37, 90000])\n",
      "torch.Size([37, 768, 90000])\n",
      "torch.Size([37, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 37it [04:18,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 90000])\n",
      "torch.Size([38, 90000])\n",
      "torch.Size([38, 768, 90000])\n",
      "torch.Size([38, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 38it [04:31, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 90000])\n",
      "torch.Size([39, 90000])\n",
      "torch.Size([39, 768, 90000])\n",
      "torch.Size([39, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 39it [04:42, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 90000])\n",
      "torch.Size([40, 90000])\n",
      "torch.Size([40, 768, 90000])\n",
      "torch.Size([40, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 40it [04:52, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 90000])\n",
      "torch.Size([41, 90000])\n",
      "torch.Size([41, 768, 90000])\n",
      "torch.Size([41, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 41it [05:03, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 90000])\n",
      "torch.Size([42, 90000])\n",
      "torch.Size([42, 768, 90000])\n",
      "torch.Size([42, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 42it [05:12, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 90000])\n",
      "torch.Size([43, 90000])\n",
      "torch.Size([43, 768, 90000])\n",
      "torch.Size([43, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 43it [05:25, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 90000])\n",
      "torch.Size([44, 90000])\n",
      "torch.Size([44, 768, 90000])\n",
      "torch.Size([44, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 44it [05:38, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 90000])\n",
      "torch.Size([45, 90000])\n",
      "torch.Size([45, 768, 90000])\n",
      "torch.Size([45, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 45it [05:50, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 90000])\n",
      "torch.Size([46, 90000])\n",
      "torch.Size([46, 768, 90000])\n",
      "torch.Size([46, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 46it [06:00, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 90000])\n",
      "torch.Size([47, 90000])\n",
      "torch.Size([47, 768, 90000])\n",
      "torch.Size([47, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 47it [06:13, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 90000])\n",
      "torch.Size([48, 90000])\n",
      "torch.Size([48, 768, 90000])\n",
      "torch.Size([48, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 48it [06:23, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 90000])\n",
      "torch.Size([49, 90000])\n",
      "torch.Size([49, 768, 90000])\n",
      "torch.Size([49, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 49it [06:37, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 90000])\n",
      "torch.Size([50, 90000])\n",
      "torch.Size([50, 768, 90000])\n",
      "torch.Size([50, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 50it [06:52, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 90000])\n",
      "torch.Size([51, 90000])\n",
      "torch.Size([51, 768, 90000])\n",
      "torch.Size([51, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoA testing: 50it [07:04,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 90000])\n",
      "torch.Size([52, 90000])\n",
      "torch.Size([52, 768, 90000])\n",
      "torch.Size([52, 3, 90000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pts, labels, soa, feat, vox = fe.test(val_loader, 50) # With the nuscenes hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fbd2fa-d395-4efb-a1f6-99c717031fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labels[1:], 'labels_test_semkitti.pt')\n",
    "torch.save(soa[1:], 'soa_test_semkitti.pt')\n",
    "torch.save(feat[1:], 'feat_test_semkitti.pt')\n",
    "torch.save(torch.tensor(vox), 'voxels_test_semkitti.pt')\n",
    "torch.save(pts[1:], 'pts_test_semkitti.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe634d-d0cc-4eeb-bca0-17cc02945efb",
   "metadata": {},
   "source": [
    "# Check if the two models have the same weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "baf51511-caba-4f91-a457-4a904b4a6aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (embed): Embedding(\n",
       "    (norm): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv1d(5, 768, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Sequential(\n",
       "      (0): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Conv2d(5, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (final): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (waffleiron): WaffleIron(\n",
       "    (channel_mix): ModuleList(\n",
       "      (0-47): 48 x ChannelMix(\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "    )\n",
       "    (spatial_mix): ModuleList(\n",
       "      (0): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (1-2): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (3): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (4-5): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (6): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (7-8): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (9): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (10-11): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (12): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (13-14): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (15): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (16-17): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (18): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (19-20): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (21): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (22-23): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (24): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (25-26): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (27): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (28-29): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (30): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (31-32): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (33): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (34-35): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (36): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (37-38): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (39): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (40-41): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (42): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (43-44): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (45): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (46-47): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classif): Sequential(\n",
       "    (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv1d(768, 19, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SemanticKitti\n",
    "fe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cdbb7b2-9119-4c90-91e9-11aeb14de3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15601/2784029278.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path_to_ckpt,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded on cuda:0: /root/main/ScaLR/saved_models/ckpt_last_scalr.pth\n"
     ]
    }
   ],
   "source": [
    "fe_2 = Feature_Extractor(nb_class=16)\n",
    "fe_2.load_pretrained('/root/main/ScaLR/saved_models/ckpt_last_scalr.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90e63368-3695-4f70-81ed-105168fe0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights differ in classif.0.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "# Assuming `model1` and `model2` are your two models\n",
    "def compare_model_weights(model1: Module, model2: Module):\n",
    "    for (name1, param1), (name2, param2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "        if name1 != name2:\n",
    "            print(f\"Parameter names differ: {name1} vs {name2}\")\n",
    "            return False\n",
    "        if not torch.equal(param1.data, param2.data):\n",
    "            print(f\"Weights differ in {name1}\")\n",
    "            return False\n",
    "    print(\"All weights are didentical!\")\n",
    "    return True\n",
    "\n",
    "# Call the function with your two models\n",
    "result = compare_model_weights(fe.model, fe_2.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1863e301-8605-4873-a531-a968dd24c91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((5,))\n",
    "F.pad(input=x, pad=(0, 5), mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e0655-8b4c-43d6-a513-f9302434cb48",
   "metadata": {},
   "source": [
    "# See nuscenes number of voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4991e33-9325-4cb4-ae57-aa771da012ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "voxels = np.load('/root/main/ScaLR/debug/nuscenes/num_voxels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615dc657-45fb-4825-a9c5-6626480f04c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18197., 16838., 18008., 17197., 17114., 16733., 17014., 16558.,\n",
       "       16587., 14200., 17857.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3103ddc3-cc1b-4a9a-a261-70d80d71e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.load('pts_train_nuscenes.pt', weights_only=\"False\")\n",
    "voxels = torch.load('voxels_train_nuscenes.pt', weights_only=\"False\")\n",
    "labels = torch.load('labels_train_nuscenes.pt', weights_only=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7133caad-7c68-44c1-9060-51ddb1f69cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16247, 4])\n",
      "torch.Size([12222, 4])\n",
      "torch.Size([14237, 4])\n",
      "torch.Size([17581, 4])\n",
      "torch.Size([17113, 4])\n",
      "torch.Size([14250, 4])\n",
      "torch.Size([15004, 4])\n",
      "torch.Size([16066, 4])\n",
      "torch.Size([16474, 4])\n",
      "torch.Size([14231, 4])\n",
      "torch.Size([16595, 4])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in range(11):\n",
    "    points_here = points[i]\n",
    "    voxels_here = voxels[i]\n",
    "    points_here = points_here[:, :voxels_here]\n",
    "    labels_here = torch.reshape(labels[i][:voxels_here], (1, points_here.shape[1]))\n",
    "    points_here = torch.cat((points_here, labels_here), dim=0)\n",
    "    points_here = torch.transpose(points_here, 0, 1)\n",
    "    print(points_here.shape)\n",
    "    # Convert to NumPy\n",
    "    tensor_np = points_here.cpu().numpy()\n",
    "    \n",
    "    # Save as CSV\n",
    "    df = pd.DataFrame(tensor_np)\n",
    "    df.to_csv(f\"tensor_nuscenes_{i}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461f504-cec1-4e83-b4be-aba19d6bdfb6",
   "metadata": {},
   "source": [
    "# Retrain Clasification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0965ed4e-02fa-4c39-b405-a7633d250169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Linear(in_features=128, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example feature size (feat_channels) and number of classes\n",
    "feat_channels = 128\n",
    "num_classes = 19\n",
    "\n",
    "# Define classification layer\n",
    "classif = nn.Linear(feat_channels, num_classes)\n",
    "\n",
    "# Define model using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.BatchNorm1d(feat_channels),\n",
    "    classif\n",
    ")\n",
    "\n",
    "# Move to device (if using GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f6b82dd-074e-49a7-ac19-48cfa005ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ba200-e97a-4b48-a408-ac530aefe66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # Set number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "    pts, labels, soa, feat, vox = fe.test(val_loader, 50) # With the nuscenes hyperparameters...\n",
    "\n",
    "    outputs = model(features)  # Forward pass\n",
    "    loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
