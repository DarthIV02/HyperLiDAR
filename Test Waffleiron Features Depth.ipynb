{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd417ca",
   "metadata": {},
   "source": [
    "# Get WaffleIron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afc500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.scatter_reduce for 3D to 2D projection.\n",
      "Using torch.scatter_reduce for 3D to 2D projection.\n"
     ]
    }
   ],
   "source": [
    "from models.waffleiron.segmenter import Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12df3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter(\n",
    "    input_channels=5,\n",
    "    feat_channels=768,\n",
    "    depth=48,\n",
    "    grid_shape=[[256, 256], [256, 32], [256, 32]],\n",
    "    nb_class=16, # class for prediction\n",
    "    #drop_path_prob=config[\"waffleiron\"][\"drop_path\"],\n",
    "    layer_norm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7e82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84939/235293898.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('/root/main/ScaLR/saved_models/ckpt_last_scalr.pth', map_location=\"cuda:0\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load pretrained model\n",
    "ckpt = torch.load('/root/main/ScaLR/saved_models/ckpt_last_scalr.pth', map_location=\"cuda:0\")\n",
    "ckpt = ckpt[\"net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4933ff55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module.embed.norm.weight', 'module.embed.norm.bias', 'module.embed.norm.running_mean', 'module.embed.norm.running_var', 'module.embed.norm.num_batches_tracked', 'module.embed.conv1.weight', 'module.embed.conv1.bias', 'module.embed.conv2.0.weight', 'module.embed.conv2.0.bias', 'module.embed.conv2.0.running_mean', 'module.embed.conv2.0.running_var', 'module.embed.conv2.0.num_batches_tracked', 'module.embed.conv2.1.weight', 'module.embed.conv2.2.weight', 'module.embed.conv2.2.bias', 'module.embed.conv2.2.running_mean', 'module.embed.conv2.2.running_var', 'module.embed.conv2.2.num_batches_tracked', 'module.embed.conv2.4.weight', 'module.embed.final.weight', 'module.embed.final.bias', 'module.waffleiron.channel_mix.0.scale.weight', 'module.waffleiron.channel_mix.0.norm.weight', 'module.waffleiron.channel_mix.0.norm.bias', 'module.waffleiron.channel_mix.0.mlp.0.weight', 'module.waffleiron.channel_mix.0.mlp.0.bias', 'module.waffleiron.channel_mix.0.mlp.2.weight', 'module.waffleiron.channel_mix.0.mlp.2.bias', 'module.waffleiron.channel_mix.1.scale.weight', 'module.waffleiron.channel_mix.1.norm.weight', 'module.waffleiron.channel_mix.1.norm.bias', 'module.waffleiron.channel_mix.1.mlp.0.weight', 'module.waffleiron.channel_mix.1.mlp.0.bias', 'module.waffleiron.channel_mix.1.mlp.2.weight', 'module.waffleiron.channel_mix.1.mlp.2.bias', 'module.waffleiron.channel_mix.2.scale.weight', 'module.waffleiron.channel_mix.2.norm.weight', 'module.waffleiron.channel_mix.2.norm.bias', 'module.waffleiron.channel_mix.2.mlp.0.weight', 'module.waffleiron.channel_mix.2.mlp.0.bias', 'module.waffleiron.channel_mix.2.mlp.2.weight', 'module.waffleiron.channel_mix.2.mlp.2.bias', 'module.waffleiron.channel_mix.3.scale.weight', 'module.waffleiron.channel_mix.3.norm.weight', 'module.waffleiron.channel_mix.3.norm.bias', 'module.waffleiron.channel_mix.3.mlp.0.weight', 'module.waffleiron.channel_mix.3.mlp.0.bias', 'module.waffleiron.channel_mix.3.mlp.2.weight', 'module.waffleiron.channel_mix.3.mlp.2.bias', 'module.waffleiron.channel_mix.4.scale.weight', 'module.waffleiron.channel_mix.4.norm.weight', 'module.waffleiron.channel_mix.4.norm.bias', 'module.waffleiron.channel_mix.4.mlp.0.weight', 'module.waffleiron.channel_mix.4.mlp.0.bias', 'module.waffleiron.channel_mix.4.mlp.2.weight', 'module.waffleiron.channel_mix.4.mlp.2.bias', 'module.waffleiron.channel_mix.5.scale.weight', 'module.waffleiron.channel_mix.5.norm.weight', 'module.waffleiron.channel_mix.5.norm.bias', 'module.waffleiron.channel_mix.5.mlp.0.weight', 'module.waffleiron.channel_mix.5.mlp.0.bias', 'module.waffleiron.channel_mix.5.mlp.2.weight', 'module.waffleiron.channel_mix.5.mlp.2.bias', 'module.waffleiron.channel_mix.6.scale.weight', 'module.waffleiron.channel_mix.6.norm.weight', 'module.waffleiron.channel_mix.6.norm.bias', 'module.waffleiron.channel_mix.6.mlp.0.weight', 'module.waffleiron.channel_mix.6.mlp.0.bias', 'module.waffleiron.channel_mix.6.mlp.2.weight', 'module.waffleiron.channel_mix.6.mlp.2.bias', 'module.waffleiron.channel_mix.7.scale.weight', 'module.waffleiron.channel_mix.7.norm.weight', 'module.waffleiron.channel_mix.7.norm.bias', 'module.waffleiron.channel_mix.7.mlp.0.weight', 'module.waffleiron.channel_mix.7.mlp.0.bias', 'module.waffleiron.channel_mix.7.mlp.2.weight', 'module.waffleiron.channel_mix.7.mlp.2.bias', 'module.waffleiron.channel_mix.8.scale.weight', 'module.waffleiron.channel_mix.8.norm.weight', 'module.waffleiron.channel_mix.8.norm.bias', 'module.waffleiron.channel_mix.8.mlp.0.weight', 'module.waffleiron.channel_mix.8.mlp.0.bias', 'module.waffleiron.channel_mix.8.mlp.2.weight', 'module.waffleiron.channel_mix.8.mlp.2.bias', 'module.waffleiron.channel_mix.9.scale.weight', 'module.waffleiron.channel_mix.9.norm.weight', 'module.waffleiron.channel_mix.9.norm.bias', 'module.waffleiron.channel_mix.9.mlp.0.weight', 'module.waffleiron.channel_mix.9.mlp.0.bias', 'module.waffleiron.channel_mix.9.mlp.2.weight', 'module.waffleiron.channel_mix.9.mlp.2.bias', 'module.waffleiron.channel_mix.10.scale.weight', 'module.waffleiron.channel_mix.10.norm.weight', 'module.waffleiron.channel_mix.10.norm.bias', 'module.waffleiron.channel_mix.10.mlp.0.weight', 'module.waffleiron.channel_mix.10.mlp.0.bias', 'module.waffleiron.channel_mix.10.mlp.2.weight', 'module.waffleiron.channel_mix.10.mlp.2.bias', 'module.waffleiron.channel_mix.11.scale.weight', 'module.waffleiron.channel_mix.11.norm.weight', 'module.waffleiron.channel_mix.11.norm.bias', 'module.waffleiron.channel_mix.11.mlp.0.weight', 'module.waffleiron.channel_mix.11.mlp.0.bias', 'module.waffleiron.channel_mix.11.mlp.2.weight', 'module.waffleiron.channel_mix.11.mlp.2.bias', 'module.waffleiron.channel_mix.12.scale.weight', 'module.waffleiron.channel_mix.12.norm.weight', 'module.waffleiron.channel_mix.12.norm.bias', 'module.waffleiron.channel_mix.12.mlp.0.weight', 'module.waffleiron.channel_mix.12.mlp.0.bias', 'module.waffleiron.channel_mix.12.mlp.2.weight', 'module.waffleiron.channel_mix.12.mlp.2.bias', 'module.waffleiron.channel_mix.13.scale.weight', 'module.waffleiron.channel_mix.13.norm.weight', 'module.waffleiron.channel_mix.13.norm.bias', 'module.waffleiron.channel_mix.13.mlp.0.weight', 'module.waffleiron.channel_mix.13.mlp.0.bias', 'module.waffleiron.channel_mix.13.mlp.2.weight', 'module.waffleiron.channel_mix.13.mlp.2.bias', 'module.waffleiron.channel_mix.14.scale.weight', 'module.waffleiron.channel_mix.14.norm.weight', 'module.waffleiron.channel_mix.14.norm.bias', 'module.waffleiron.channel_mix.14.mlp.0.weight', 'module.waffleiron.channel_mix.14.mlp.0.bias', 'module.waffleiron.channel_mix.14.mlp.2.weight', 'module.waffleiron.channel_mix.14.mlp.2.bias', 'module.waffleiron.channel_mix.15.scale.weight', 'module.waffleiron.channel_mix.15.norm.weight', 'module.waffleiron.channel_mix.15.norm.bias', 'module.waffleiron.channel_mix.15.mlp.0.weight', 'module.waffleiron.channel_mix.15.mlp.0.bias', 'module.waffleiron.channel_mix.15.mlp.2.weight', 'module.waffleiron.channel_mix.15.mlp.2.bias', 'module.waffleiron.channel_mix.16.scale.weight', 'module.waffleiron.channel_mix.16.norm.weight', 'module.waffleiron.channel_mix.16.norm.bias', 'module.waffleiron.channel_mix.16.mlp.0.weight', 'module.waffleiron.channel_mix.16.mlp.0.bias', 'module.waffleiron.channel_mix.16.mlp.2.weight', 'module.waffleiron.channel_mix.16.mlp.2.bias', 'module.waffleiron.channel_mix.17.scale.weight', 'module.waffleiron.channel_mix.17.norm.weight', 'module.waffleiron.channel_mix.17.norm.bias', 'module.waffleiron.channel_mix.17.mlp.0.weight', 'module.waffleiron.channel_mix.17.mlp.0.bias', 'module.waffleiron.channel_mix.17.mlp.2.weight', 'module.waffleiron.channel_mix.17.mlp.2.bias', 'module.waffleiron.channel_mix.18.scale.weight', 'module.waffleiron.channel_mix.18.norm.weight', 'module.waffleiron.channel_mix.18.norm.bias', 'module.waffleiron.channel_mix.18.mlp.0.weight', 'module.waffleiron.channel_mix.18.mlp.0.bias', 'module.waffleiron.channel_mix.18.mlp.2.weight', 'module.waffleiron.channel_mix.18.mlp.2.bias', 'module.waffleiron.channel_mix.19.scale.weight', 'module.waffleiron.channel_mix.19.norm.weight', 'module.waffleiron.channel_mix.19.norm.bias', 'module.waffleiron.channel_mix.19.mlp.0.weight', 'module.waffleiron.channel_mix.19.mlp.0.bias', 'module.waffleiron.channel_mix.19.mlp.2.weight', 'module.waffleiron.channel_mix.19.mlp.2.bias', 'module.waffleiron.channel_mix.20.scale.weight', 'module.waffleiron.channel_mix.20.norm.weight', 'module.waffleiron.channel_mix.20.norm.bias', 'module.waffleiron.channel_mix.20.mlp.0.weight', 'module.waffleiron.channel_mix.20.mlp.0.bias', 'module.waffleiron.channel_mix.20.mlp.2.weight', 'module.waffleiron.channel_mix.20.mlp.2.bias', 'module.waffleiron.channel_mix.21.scale.weight', 'module.waffleiron.channel_mix.21.norm.weight', 'module.waffleiron.channel_mix.21.norm.bias', 'module.waffleiron.channel_mix.21.mlp.0.weight', 'module.waffleiron.channel_mix.21.mlp.0.bias', 'module.waffleiron.channel_mix.21.mlp.2.weight', 'module.waffleiron.channel_mix.21.mlp.2.bias', 'module.waffleiron.channel_mix.22.scale.weight', 'module.waffleiron.channel_mix.22.norm.weight', 'module.waffleiron.channel_mix.22.norm.bias', 'module.waffleiron.channel_mix.22.mlp.0.weight', 'module.waffleiron.channel_mix.22.mlp.0.bias', 'module.waffleiron.channel_mix.22.mlp.2.weight', 'module.waffleiron.channel_mix.22.mlp.2.bias', 'module.waffleiron.channel_mix.23.scale.weight', 'module.waffleiron.channel_mix.23.norm.weight', 'module.waffleiron.channel_mix.23.norm.bias', 'module.waffleiron.channel_mix.23.mlp.0.weight', 'module.waffleiron.channel_mix.23.mlp.0.bias', 'module.waffleiron.channel_mix.23.mlp.2.weight', 'module.waffleiron.channel_mix.23.mlp.2.bias', 'module.waffleiron.channel_mix.24.scale.weight', 'module.waffleiron.channel_mix.24.norm.weight', 'module.waffleiron.channel_mix.24.norm.bias', 'module.waffleiron.channel_mix.24.mlp.0.weight', 'module.waffleiron.channel_mix.24.mlp.0.bias', 'module.waffleiron.channel_mix.24.mlp.2.weight', 'module.waffleiron.channel_mix.24.mlp.2.bias', 'module.waffleiron.channel_mix.25.scale.weight', 'module.waffleiron.channel_mix.25.norm.weight', 'module.waffleiron.channel_mix.25.norm.bias', 'module.waffleiron.channel_mix.25.mlp.0.weight', 'module.waffleiron.channel_mix.25.mlp.0.bias', 'module.waffleiron.channel_mix.25.mlp.2.weight', 'module.waffleiron.channel_mix.25.mlp.2.bias', 'module.waffleiron.channel_mix.26.scale.weight', 'module.waffleiron.channel_mix.26.norm.weight', 'module.waffleiron.channel_mix.26.norm.bias', 'module.waffleiron.channel_mix.26.mlp.0.weight', 'module.waffleiron.channel_mix.26.mlp.0.bias', 'module.waffleiron.channel_mix.26.mlp.2.weight', 'module.waffleiron.channel_mix.26.mlp.2.bias', 'module.waffleiron.channel_mix.27.scale.weight', 'module.waffleiron.channel_mix.27.norm.weight', 'module.waffleiron.channel_mix.27.norm.bias', 'module.waffleiron.channel_mix.27.mlp.0.weight', 'module.waffleiron.channel_mix.27.mlp.0.bias', 'module.waffleiron.channel_mix.27.mlp.2.weight', 'module.waffleiron.channel_mix.27.mlp.2.bias', 'module.waffleiron.channel_mix.28.scale.weight', 'module.waffleiron.channel_mix.28.norm.weight', 'module.waffleiron.channel_mix.28.norm.bias', 'module.waffleiron.channel_mix.28.mlp.0.weight', 'module.waffleiron.channel_mix.28.mlp.0.bias', 'module.waffleiron.channel_mix.28.mlp.2.weight', 'module.waffleiron.channel_mix.28.mlp.2.bias', 'module.waffleiron.channel_mix.29.scale.weight', 'module.waffleiron.channel_mix.29.norm.weight', 'module.waffleiron.channel_mix.29.norm.bias', 'module.waffleiron.channel_mix.29.mlp.0.weight', 'module.waffleiron.channel_mix.29.mlp.0.bias', 'module.waffleiron.channel_mix.29.mlp.2.weight', 'module.waffleiron.channel_mix.29.mlp.2.bias', 'module.waffleiron.channel_mix.30.scale.weight', 'module.waffleiron.channel_mix.30.norm.weight', 'module.waffleiron.channel_mix.30.norm.bias', 'module.waffleiron.channel_mix.30.mlp.0.weight', 'module.waffleiron.channel_mix.30.mlp.0.bias', 'module.waffleiron.channel_mix.30.mlp.2.weight', 'module.waffleiron.channel_mix.30.mlp.2.bias', 'module.waffleiron.channel_mix.31.scale.weight', 'module.waffleiron.channel_mix.31.norm.weight', 'module.waffleiron.channel_mix.31.norm.bias', 'module.waffleiron.channel_mix.31.mlp.0.weight', 'module.waffleiron.channel_mix.31.mlp.0.bias', 'module.waffleiron.channel_mix.31.mlp.2.weight', 'module.waffleiron.channel_mix.31.mlp.2.bias', 'module.waffleiron.channel_mix.32.scale.weight', 'module.waffleiron.channel_mix.32.norm.weight', 'module.waffleiron.channel_mix.32.norm.bias', 'module.waffleiron.channel_mix.32.mlp.0.weight', 'module.waffleiron.channel_mix.32.mlp.0.bias', 'module.waffleiron.channel_mix.32.mlp.2.weight', 'module.waffleiron.channel_mix.32.mlp.2.bias', 'module.waffleiron.channel_mix.33.scale.weight', 'module.waffleiron.channel_mix.33.norm.weight', 'module.waffleiron.channel_mix.33.norm.bias', 'module.waffleiron.channel_mix.33.mlp.0.weight', 'module.waffleiron.channel_mix.33.mlp.0.bias', 'module.waffleiron.channel_mix.33.mlp.2.weight', 'module.waffleiron.channel_mix.33.mlp.2.bias', 'module.waffleiron.channel_mix.34.scale.weight', 'module.waffleiron.channel_mix.34.norm.weight', 'module.waffleiron.channel_mix.34.norm.bias', 'module.waffleiron.channel_mix.34.mlp.0.weight', 'module.waffleiron.channel_mix.34.mlp.0.bias', 'module.waffleiron.channel_mix.34.mlp.2.weight', 'module.waffleiron.channel_mix.34.mlp.2.bias', 'module.waffleiron.channel_mix.35.scale.weight', 'module.waffleiron.channel_mix.35.norm.weight', 'module.waffleiron.channel_mix.35.norm.bias', 'module.waffleiron.channel_mix.35.mlp.0.weight', 'module.waffleiron.channel_mix.35.mlp.0.bias', 'module.waffleiron.channel_mix.35.mlp.2.weight', 'module.waffleiron.channel_mix.35.mlp.2.bias', 'module.waffleiron.channel_mix.36.scale.weight', 'module.waffleiron.channel_mix.36.norm.weight', 'module.waffleiron.channel_mix.36.norm.bias', 'module.waffleiron.channel_mix.36.mlp.0.weight', 'module.waffleiron.channel_mix.36.mlp.0.bias', 'module.waffleiron.channel_mix.36.mlp.2.weight', 'module.waffleiron.channel_mix.36.mlp.2.bias', 'module.waffleiron.channel_mix.37.scale.weight', 'module.waffleiron.channel_mix.37.norm.weight', 'module.waffleiron.channel_mix.37.norm.bias', 'module.waffleiron.channel_mix.37.mlp.0.weight', 'module.waffleiron.channel_mix.37.mlp.0.bias', 'module.waffleiron.channel_mix.37.mlp.2.weight', 'module.waffleiron.channel_mix.37.mlp.2.bias', 'module.waffleiron.channel_mix.38.scale.weight', 'module.waffleiron.channel_mix.38.norm.weight', 'module.waffleiron.channel_mix.38.norm.bias', 'module.waffleiron.channel_mix.38.mlp.0.weight', 'module.waffleiron.channel_mix.38.mlp.0.bias', 'module.waffleiron.channel_mix.38.mlp.2.weight', 'module.waffleiron.channel_mix.38.mlp.2.bias', 'module.waffleiron.channel_mix.39.scale.weight', 'module.waffleiron.channel_mix.39.norm.weight', 'module.waffleiron.channel_mix.39.norm.bias', 'module.waffleiron.channel_mix.39.mlp.0.weight', 'module.waffleiron.channel_mix.39.mlp.0.bias', 'module.waffleiron.channel_mix.39.mlp.2.weight', 'module.waffleiron.channel_mix.39.mlp.2.bias', 'module.waffleiron.channel_mix.40.scale.weight', 'module.waffleiron.channel_mix.40.norm.weight', 'module.waffleiron.channel_mix.40.norm.bias', 'module.waffleiron.channel_mix.40.mlp.0.weight', 'module.waffleiron.channel_mix.40.mlp.0.bias', 'module.waffleiron.channel_mix.40.mlp.2.weight', 'module.waffleiron.channel_mix.40.mlp.2.bias', 'module.waffleiron.channel_mix.41.scale.weight', 'module.waffleiron.channel_mix.41.norm.weight', 'module.waffleiron.channel_mix.41.norm.bias', 'module.waffleiron.channel_mix.41.mlp.0.weight', 'module.waffleiron.channel_mix.41.mlp.0.bias', 'module.waffleiron.channel_mix.41.mlp.2.weight', 'module.waffleiron.channel_mix.41.mlp.2.bias', 'module.waffleiron.channel_mix.42.scale.weight', 'module.waffleiron.channel_mix.42.norm.weight', 'module.waffleiron.channel_mix.42.norm.bias', 'module.waffleiron.channel_mix.42.mlp.0.weight', 'module.waffleiron.channel_mix.42.mlp.0.bias', 'module.waffleiron.channel_mix.42.mlp.2.weight', 'module.waffleiron.channel_mix.42.mlp.2.bias', 'module.waffleiron.channel_mix.43.scale.weight', 'module.waffleiron.channel_mix.43.norm.weight', 'module.waffleiron.channel_mix.43.norm.bias', 'module.waffleiron.channel_mix.43.mlp.0.weight', 'module.waffleiron.channel_mix.43.mlp.0.bias', 'module.waffleiron.channel_mix.43.mlp.2.weight', 'module.waffleiron.channel_mix.43.mlp.2.bias', 'module.waffleiron.channel_mix.44.scale.weight', 'module.waffleiron.channel_mix.44.norm.weight', 'module.waffleiron.channel_mix.44.norm.bias', 'module.waffleiron.channel_mix.44.mlp.0.weight', 'module.waffleiron.channel_mix.44.mlp.0.bias', 'module.waffleiron.channel_mix.44.mlp.2.weight', 'module.waffleiron.channel_mix.44.mlp.2.bias', 'module.waffleiron.channel_mix.45.scale.weight', 'module.waffleiron.channel_mix.45.norm.weight', 'module.waffleiron.channel_mix.45.norm.bias', 'module.waffleiron.channel_mix.45.mlp.0.weight', 'module.waffleiron.channel_mix.45.mlp.0.bias', 'module.waffleiron.channel_mix.45.mlp.2.weight', 'module.waffleiron.channel_mix.45.mlp.2.bias', 'module.waffleiron.channel_mix.46.scale.weight', 'module.waffleiron.channel_mix.46.norm.weight', 'module.waffleiron.channel_mix.46.norm.bias', 'module.waffleiron.channel_mix.46.mlp.0.weight', 'module.waffleiron.channel_mix.46.mlp.0.bias', 'module.waffleiron.channel_mix.46.mlp.2.weight', 'module.waffleiron.channel_mix.46.mlp.2.bias', 'module.waffleiron.channel_mix.47.scale.weight', 'module.waffleiron.channel_mix.47.norm.weight', 'module.waffleiron.channel_mix.47.norm.bias', 'module.waffleiron.channel_mix.47.mlp.0.weight', 'module.waffleiron.channel_mix.47.mlp.0.bias', 'module.waffleiron.channel_mix.47.mlp.2.weight', 'module.waffleiron.channel_mix.47.mlp.2.bias', 'module.waffleiron.spatial_mix.0.scale.weight', 'module.waffleiron.spatial_mix.0.norm.weight', 'module.waffleiron.spatial_mix.0.norm.bias', 'module.waffleiron.spatial_mix.0.ffn.0.weight', 'module.waffleiron.spatial_mix.0.ffn.0.bias', 'module.waffleiron.spatial_mix.0.ffn.2.weight', 'module.waffleiron.spatial_mix.0.ffn.2.bias', 'module.waffleiron.spatial_mix.1.scale.weight', 'module.waffleiron.spatial_mix.1.norm.weight', 'module.waffleiron.spatial_mix.1.norm.bias', 'module.waffleiron.spatial_mix.1.ffn.0.weight', 'module.waffleiron.spatial_mix.1.ffn.0.bias', 'module.waffleiron.spatial_mix.1.ffn.2.weight', 'module.waffleiron.spatial_mix.1.ffn.2.bias', 'module.waffleiron.spatial_mix.2.scale.weight', 'module.waffleiron.spatial_mix.2.norm.weight', 'module.waffleiron.spatial_mix.2.norm.bias', 'module.waffleiron.spatial_mix.2.ffn.0.weight', 'module.waffleiron.spatial_mix.2.ffn.0.bias', 'module.waffleiron.spatial_mix.2.ffn.2.weight', 'module.waffleiron.spatial_mix.2.ffn.2.bias', 'module.waffleiron.spatial_mix.3.scale.weight', 'module.waffleiron.spatial_mix.3.norm.weight', 'module.waffleiron.spatial_mix.3.norm.bias', 'module.waffleiron.spatial_mix.3.ffn.0.weight', 'module.waffleiron.spatial_mix.3.ffn.0.bias', 'module.waffleiron.spatial_mix.3.ffn.2.weight', 'module.waffleiron.spatial_mix.3.ffn.2.bias', 'module.waffleiron.spatial_mix.4.scale.weight', 'module.waffleiron.spatial_mix.4.norm.weight', 'module.waffleiron.spatial_mix.4.norm.bias', 'module.waffleiron.spatial_mix.4.ffn.0.weight', 'module.waffleiron.spatial_mix.4.ffn.0.bias', 'module.waffleiron.spatial_mix.4.ffn.2.weight', 'module.waffleiron.spatial_mix.4.ffn.2.bias', 'module.waffleiron.spatial_mix.5.scale.weight', 'module.waffleiron.spatial_mix.5.norm.weight', 'module.waffleiron.spatial_mix.5.norm.bias', 'module.waffleiron.spatial_mix.5.ffn.0.weight', 'module.waffleiron.spatial_mix.5.ffn.0.bias', 'module.waffleiron.spatial_mix.5.ffn.2.weight', 'module.waffleiron.spatial_mix.5.ffn.2.bias', 'module.waffleiron.spatial_mix.6.scale.weight', 'module.waffleiron.spatial_mix.6.norm.weight', 'module.waffleiron.spatial_mix.6.norm.bias', 'module.waffleiron.spatial_mix.6.ffn.0.weight', 'module.waffleiron.spatial_mix.6.ffn.0.bias', 'module.waffleiron.spatial_mix.6.ffn.2.weight', 'module.waffleiron.spatial_mix.6.ffn.2.bias', 'module.waffleiron.spatial_mix.7.scale.weight', 'module.waffleiron.spatial_mix.7.norm.weight', 'module.waffleiron.spatial_mix.7.norm.bias', 'module.waffleiron.spatial_mix.7.ffn.0.weight', 'module.waffleiron.spatial_mix.7.ffn.0.bias', 'module.waffleiron.spatial_mix.7.ffn.2.weight', 'module.waffleiron.spatial_mix.7.ffn.2.bias', 'module.waffleiron.spatial_mix.8.scale.weight', 'module.waffleiron.spatial_mix.8.norm.weight', 'module.waffleiron.spatial_mix.8.norm.bias', 'module.waffleiron.spatial_mix.8.ffn.0.weight', 'module.waffleiron.spatial_mix.8.ffn.0.bias', 'module.waffleiron.spatial_mix.8.ffn.2.weight', 'module.waffleiron.spatial_mix.8.ffn.2.bias', 'module.waffleiron.spatial_mix.9.scale.weight', 'module.waffleiron.spatial_mix.9.norm.weight', 'module.waffleiron.spatial_mix.9.norm.bias', 'module.waffleiron.spatial_mix.9.ffn.0.weight', 'module.waffleiron.spatial_mix.9.ffn.0.bias', 'module.waffleiron.spatial_mix.9.ffn.2.weight', 'module.waffleiron.spatial_mix.9.ffn.2.bias', 'module.waffleiron.spatial_mix.10.scale.weight', 'module.waffleiron.spatial_mix.10.norm.weight', 'module.waffleiron.spatial_mix.10.norm.bias', 'module.waffleiron.spatial_mix.10.ffn.0.weight', 'module.waffleiron.spatial_mix.10.ffn.0.bias', 'module.waffleiron.spatial_mix.10.ffn.2.weight', 'module.waffleiron.spatial_mix.10.ffn.2.bias', 'module.waffleiron.spatial_mix.11.scale.weight', 'module.waffleiron.spatial_mix.11.norm.weight', 'module.waffleiron.spatial_mix.11.norm.bias', 'module.waffleiron.spatial_mix.11.ffn.0.weight', 'module.waffleiron.spatial_mix.11.ffn.0.bias', 'module.waffleiron.spatial_mix.11.ffn.2.weight', 'module.waffleiron.spatial_mix.11.ffn.2.bias', 'module.waffleiron.spatial_mix.12.scale.weight', 'module.waffleiron.spatial_mix.12.norm.weight', 'module.waffleiron.spatial_mix.12.norm.bias', 'module.waffleiron.spatial_mix.12.ffn.0.weight', 'module.waffleiron.spatial_mix.12.ffn.0.bias', 'module.waffleiron.spatial_mix.12.ffn.2.weight', 'module.waffleiron.spatial_mix.12.ffn.2.bias', 'module.waffleiron.spatial_mix.13.scale.weight', 'module.waffleiron.spatial_mix.13.norm.weight', 'module.waffleiron.spatial_mix.13.norm.bias', 'module.waffleiron.spatial_mix.13.ffn.0.weight', 'module.waffleiron.spatial_mix.13.ffn.0.bias', 'module.waffleiron.spatial_mix.13.ffn.2.weight', 'module.waffleiron.spatial_mix.13.ffn.2.bias', 'module.waffleiron.spatial_mix.14.scale.weight', 'module.waffleiron.spatial_mix.14.norm.weight', 'module.waffleiron.spatial_mix.14.norm.bias', 'module.waffleiron.spatial_mix.14.ffn.0.weight', 'module.waffleiron.spatial_mix.14.ffn.0.bias', 'module.waffleiron.spatial_mix.14.ffn.2.weight', 'module.waffleiron.spatial_mix.14.ffn.2.bias', 'module.waffleiron.spatial_mix.15.scale.weight', 'module.waffleiron.spatial_mix.15.norm.weight', 'module.waffleiron.spatial_mix.15.norm.bias', 'module.waffleiron.spatial_mix.15.ffn.0.weight', 'module.waffleiron.spatial_mix.15.ffn.0.bias', 'module.waffleiron.spatial_mix.15.ffn.2.weight', 'module.waffleiron.spatial_mix.15.ffn.2.bias', 'module.waffleiron.spatial_mix.16.scale.weight', 'module.waffleiron.spatial_mix.16.norm.weight', 'module.waffleiron.spatial_mix.16.norm.bias', 'module.waffleiron.spatial_mix.16.ffn.0.weight', 'module.waffleiron.spatial_mix.16.ffn.0.bias', 'module.waffleiron.spatial_mix.16.ffn.2.weight', 'module.waffleiron.spatial_mix.16.ffn.2.bias', 'module.waffleiron.spatial_mix.17.scale.weight', 'module.waffleiron.spatial_mix.17.norm.weight', 'module.waffleiron.spatial_mix.17.norm.bias', 'module.waffleiron.spatial_mix.17.ffn.0.weight', 'module.waffleiron.spatial_mix.17.ffn.0.bias', 'module.waffleiron.spatial_mix.17.ffn.2.weight', 'module.waffleiron.spatial_mix.17.ffn.2.bias', 'module.waffleiron.spatial_mix.18.scale.weight', 'module.waffleiron.spatial_mix.18.norm.weight', 'module.waffleiron.spatial_mix.18.norm.bias', 'module.waffleiron.spatial_mix.18.ffn.0.weight', 'module.waffleiron.spatial_mix.18.ffn.0.bias', 'module.waffleiron.spatial_mix.18.ffn.2.weight', 'module.waffleiron.spatial_mix.18.ffn.2.bias', 'module.waffleiron.spatial_mix.19.scale.weight', 'module.waffleiron.spatial_mix.19.norm.weight', 'module.waffleiron.spatial_mix.19.norm.bias', 'module.waffleiron.spatial_mix.19.ffn.0.weight', 'module.waffleiron.spatial_mix.19.ffn.0.bias', 'module.waffleiron.spatial_mix.19.ffn.2.weight', 'module.waffleiron.spatial_mix.19.ffn.2.bias', 'module.waffleiron.spatial_mix.20.scale.weight', 'module.waffleiron.spatial_mix.20.norm.weight', 'module.waffleiron.spatial_mix.20.norm.bias', 'module.waffleiron.spatial_mix.20.ffn.0.weight', 'module.waffleiron.spatial_mix.20.ffn.0.bias', 'module.waffleiron.spatial_mix.20.ffn.2.weight', 'module.waffleiron.spatial_mix.20.ffn.2.bias', 'module.waffleiron.spatial_mix.21.scale.weight', 'module.waffleiron.spatial_mix.21.norm.weight', 'module.waffleiron.spatial_mix.21.norm.bias', 'module.waffleiron.spatial_mix.21.ffn.0.weight', 'module.waffleiron.spatial_mix.21.ffn.0.bias', 'module.waffleiron.spatial_mix.21.ffn.2.weight', 'module.waffleiron.spatial_mix.21.ffn.2.bias', 'module.waffleiron.spatial_mix.22.scale.weight', 'module.waffleiron.spatial_mix.22.norm.weight', 'module.waffleiron.spatial_mix.22.norm.bias', 'module.waffleiron.spatial_mix.22.ffn.0.weight', 'module.waffleiron.spatial_mix.22.ffn.0.bias', 'module.waffleiron.spatial_mix.22.ffn.2.weight', 'module.waffleiron.spatial_mix.22.ffn.2.bias', 'module.waffleiron.spatial_mix.23.scale.weight', 'module.waffleiron.spatial_mix.23.norm.weight', 'module.waffleiron.spatial_mix.23.norm.bias', 'module.waffleiron.spatial_mix.23.ffn.0.weight', 'module.waffleiron.spatial_mix.23.ffn.0.bias', 'module.waffleiron.spatial_mix.23.ffn.2.weight', 'module.waffleiron.spatial_mix.23.ffn.2.bias', 'module.waffleiron.spatial_mix.24.scale.weight', 'module.waffleiron.spatial_mix.24.norm.weight', 'module.waffleiron.spatial_mix.24.norm.bias', 'module.waffleiron.spatial_mix.24.ffn.0.weight', 'module.waffleiron.spatial_mix.24.ffn.0.bias', 'module.waffleiron.spatial_mix.24.ffn.2.weight', 'module.waffleiron.spatial_mix.24.ffn.2.bias', 'module.waffleiron.spatial_mix.25.scale.weight', 'module.waffleiron.spatial_mix.25.norm.weight', 'module.waffleiron.spatial_mix.25.norm.bias', 'module.waffleiron.spatial_mix.25.ffn.0.weight', 'module.waffleiron.spatial_mix.25.ffn.0.bias', 'module.waffleiron.spatial_mix.25.ffn.2.weight', 'module.waffleiron.spatial_mix.25.ffn.2.bias', 'module.waffleiron.spatial_mix.26.scale.weight', 'module.waffleiron.spatial_mix.26.norm.weight', 'module.waffleiron.spatial_mix.26.norm.bias', 'module.waffleiron.spatial_mix.26.ffn.0.weight', 'module.waffleiron.spatial_mix.26.ffn.0.bias', 'module.waffleiron.spatial_mix.26.ffn.2.weight', 'module.waffleiron.spatial_mix.26.ffn.2.bias', 'module.waffleiron.spatial_mix.27.scale.weight', 'module.waffleiron.spatial_mix.27.norm.weight', 'module.waffleiron.spatial_mix.27.norm.bias', 'module.waffleiron.spatial_mix.27.ffn.0.weight', 'module.waffleiron.spatial_mix.27.ffn.0.bias', 'module.waffleiron.spatial_mix.27.ffn.2.weight', 'module.waffleiron.spatial_mix.27.ffn.2.bias', 'module.waffleiron.spatial_mix.28.scale.weight', 'module.waffleiron.spatial_mix.28.norm.weight', 'module.waffleiron.spatial_mix.28.norm.bias', 'module.waffleiron.spatial_mix.28.ffn.0.weight', 'module.waffleiron.spatial_mix.28.ffn.0.bias', 'module.waffleiron.spatial_mix.28.ffn.2.weight', 'module.waffleiron.spatial_mix.28.ffn.2.bias', 'module.waffleiron.spatial_mix.29.scale.weight', 'module.waffleiron.spatial_mix.29.norm.weight', 'module.waffleiron.spatial_mix.29.norm.bias', 'module.waffleiron.spatial_mix.29.ffn.0.weight', 'module.waffleiron.spatial_mix.29.ffn.0.bias', 'module.waffleiron.spatial_mix.29.ffn.2.weight', 'module.waffleiron.spatial_mix.29.ffn.2.bias', 'module.waffleiron.spatial_mix.30.scale.weight', 'module.waffleiron.spatial_mix.30.norm.weight', 'module.waffleiron.spatial_mix.30.norm.bias', 'module.waffleiron.spatial_mix.30.ffn.0.weight', 'module.waffleiron.spatial_mix.30.ffn.0.bias', 'module.waffleiron.spatial_mix.30.ffn.2.weight', 'module.waffleiron.spatial_mix.30.ffn.2.bias', 'module.waffleiron.spatial_mix.31.scale.weight', 'module.waffleiron.spatial_mix.31.norm.weight', 'module.waffleiron.spatial_mix.31.norm.bias', 'module.waffleiron.spatial_mix.31.ffn.0.weight', 'module.waffleiron.spatial_mix.31.ffn.0.bias', 'module.waffleiron.spatial_mix.31.ffn.2.weight', 'module.waffleiron.spatial_mix.31.ffn.2.bias', 'module.waffleiron.spatial_mix.32.scale.weight', 'module.waffleiron.spatial_mix.32.norm.weight', 'module.waffleiron.spatial_mix.32.norm.bias', 'module.waffleiron.spatial_mix.32.ffn.0.weight', 'module.waffleiron.spatial_mix.32.ffn.0.bias', 'module.waffleiron.spatial_mix.32.ffn.2.weight', 'module.waffleiron.spatial_mix.32.ffn.2.bias', 'module.waffleiron.spatial_mix.33.scale.weight', 'module.waffleiron.spatial_mix.33.norm.weight', 'module.waffleiron.spatial_mix.33.norm.bias', 'module.waffleiron.spatial_mix.33.ffn.0.weight', 'module.waffleiron.spatial_mix.33.ffn.0.bias', 'module.waffleiron.spatial_mix.33.ffn.2.weight', 'module.waffleiron.spatial_mix.33.ffn.2.bias', 'module.waffleiron.spatial_mix.34.scale.weight', 'module.waffleiron.spatial_mix.34.norm.weight', 'module.waffleiron.spatial_mix.34.norm.bias', 'module.waffleiron.spatial_mix.34.ffn.0.weight', 'module.waffleiron.spatial_mix.34.ffn.0.bias', 'module.waffleiron.spatial_mix.34.ffn.2.weight', 'module.waffleiron.spatial_mix.34.ffn.2.bias', 'module.waffleiron.spatial_mix.35.scale.weight', 'module.waffleiron.spatial_mix.35.norm.weight', 'module.waffleiron.spatial_mix.35.norm.bias', 'module.waffleiron.spatial_mix.35.ffn.0.weight', 'module.waffleiron.spatial_mix.35.ffn.0.bias', 'module.waffleiron.spatial_mix.35.ffn.2.weight', 'module.waffleiron.spatial_mix.35.ffn.2.bias', 'module.waffleiron.spatial_mix.36.scale.weight', 'module.waffleiron.spatial_mix.36.norm.weight', 'module.waffleiron.spatial_mix.36.norm.bias', 'module.waffleiron.spatial_mix.36.ffn.0.weight', 'module.waffleiron.spatial_mix.36.ffn.0.bias', 'module.waffleiron.spatial_mix.36.ffn.2.weight', 'module.waffleiron.spatial_mix.36.ffn.2.bias', 'module.waffleiron.spatial_mix.37.scale.weight', 'module.waffleiron.spatial_mix.37.norm.weight', 'module.waffleiron.spatial_mix.37.norm.bias', 'module.waffleiron.spatial_mix.37.ffn.0.weight', 'module.waffleiron.spatial_mix.37.ffn.0.bias', 'module.waffleiron.spatial_mix.37.ffn.2.weight', 'module.waffleiron.spatial_mix.37.ffn.2.bias', 'module.waffleiron.spatial_mix.38.scale.weight', 'module.waffleiron.spatial_mix.38.norm.weight', 'module.waffleiron.spatial_mix.38.norm.bias', 'module.waffleiron.spatial_mix.38.ffn.0.weight', 'module.waffleiron.spatial_mix.38.ffn.0.bias', 'module.waffleiron.spatial_mix.38.ffn.2.weight', 'module.waffleiron.spatial_mix.38.ffn.2.bias', 'module.waffleiron.spatial_mix.39.scale.weight', 'module.waffleiron.spatial_mix.39.norm.weight', 'module.waffleiron.spatial_mix.39.norm.bias', 'module.waffleiron.spatial_mix.39.ffn.0.weight', 'module.waffleiron.spatial_mix.39.ffn.0.bias', 'module.waffleiron.spatial_mix.39.ffn.2.weight', 'module.waffleiron.spatial_mix.39.ffn.2.bias', 'module.waffleiron.spatial_mix.40.scale.weight', 'module.waffleiron.spatial_mix.40.norm.weight', 'module.waffleiron.spatial_mix.40.norm.bias', 'module.waffleiron.spatial_mix.40.ffn.0.weight', 'module.waffleiron.spatial_mix.40.ffn.0.bias', 'module.waffleiron.spatial_mix.40.ffn.2.weight', 'module.waffleiron.spatial_mix.40.ffn.2.bias', 'module.waffleiron.spatial_mix.41.scale.weight', 'module.waffleiron.spatial_mix.41.norm.weight', 'module.waffleiron.spatial_mix.41.norm.bias', 'module.waffleiron.spatial_mix.41.ffn.0.weight', 'module.waffleiron.spatial_mix.41.ffn.0.bias', 'module.waffleiron.spatial_mix.41.ffn.2.weight', 'module.waffleiron.spatial_mix.41.ffn.2.bias', 'module.waffleiron.spatial_mix.42.scale.weight', 'module.waffleiron.spatial_mix.42.norm.weight', 'module.waffleiron.spatial_mix.42.norm.bias', 'module.waffleiron.spatial_mix.42.ffn.0.weight', 'module.waffleiron.spatial_mix.42.ffn.0.bias', 'module.waffleiron.spatial_mix.42.ffn.2.weight', 'module.waffleiron.spatial_mix.42.ffn.2.bias', 'module.waffleiron.spatial_mix.43.scale.weight', 'module.waffleiron.spatial_mix.43.norm.weight', 'module.waffleiron.spatial_mix.43.norm.bias', 'module.waffleiron.spatial_mix.43.ffn.0.weight', 'module.waffleiron.spatial_mix.43.ffn.0.bias', 'module.waffleiron.spatial_mix.43.ffn.2.weight', 'module.waffleiron.spatial_mix.43.ffn.2.bias', 'module.waffleiron.spatial_mix.44.scale.weight', 'module.waffleiron.spatial_mix.44.norm.weight', 'module.waffleiron.spatial_mix.44.norm.bias', 'module.waffleiron.spatial_mix.44.ffn.0.weight', 'module.waffleiron.spatial_mix.44.ffn.0.bias', 'module.waffleiron.spatial_mix.44.ffn.2.weight', 'module.waffleiron.spatial_mix.44.ffn.2.bias', 'module.waffleiron.spatial_mix.45.scale.weight', 'module.waffleiron.spatial_mix.45.norm.weight', 'module.waffleiron.spatial_mix.45.norm.bias', 'module.waffleiron.spatial_mix.45.ffn.0.weight', 'module.waffleiron.spatial_mix.45.ffn.0.bias', 'module.waffleiron.spatial_mix.45.ffn.2.weight', 'module.waffleiron.spatial_mix.45.ffn.2.bias', 'module.waffleiron.spatial_mix.46.scale.weight', 'module.waffleiron.spatial_mix.46.norm.weight', 'module.waffleiron.spatial_mix.46.norm.bias', 'module.waffleiron.spatial_mix.46.ffn.0.weight', 'module.waffleiron.spatial_mix.46.ffn.0.bias', 'module.waffleiron.spatial_mix.46.ffn.2.weight', 'module.waffleiron.spatial_mix.46.ffn.2.bias', 'module.waffleiron.spatial_mix.47.scale.weight', 'module.waffleiron.spatial_mix.47.norm.weight', 'module.waffleiron.spatial_mix.47.norm.bias', 'module.waffleiron.spatial_mix.47.ffn.0.weight', 'module.waffleiron.spatial_mix.47.ffn.0.bias', 'module.waffleiron.spatial_mix.47.ffn.2.weight', 'module.waffleiron.spatial_mix.47.ffn.2.bias', 'module.classif.0.weight', 'module.classif.0.bias', 'module.classif.0.running_mean', 'module.classif.0.running_var', 'module.classif.0.num_batches_tracked', 'module.classif.1.weight', 'module.classif.1.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c587871",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ckpt = {}\n",
    "for k in ckpt.keys():\n",
    "    if k.startswith(\"module\"):\n",
    "        if k.startswith(\"module.classif.0\"):\n",
    "            continue\n",
    "        elif k.startswith(\"module.classif.1\"):\n",
    "            new_ckpt[\"classif\" + k[len(\"module.classif.1\") :]] = ckpt[k]\n",
    "        else:\n",
    "            new_ckpt[k[len(\"module.\") :]] = ckpt[k]\n",
    "    else:\n",
    "        new_ckpt[k] = ckpt[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a10e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ckpt.get(\"classif.weight\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca00085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f2c3d",
   "metadata": {},
   "source": [
    "## Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c4ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(\"cuda:0\")\n",
    "model = model.cuda(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e309a941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (embed): Embedding(\n",
       "    (norm): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv1d(5, 768, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Sequential(\n",
       "      (0): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Conv2d(5, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (final): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (waffleiron): WaffleIron(\n",
       "    (channel_mix): ModuleList(\n",
       "      (0-47): 48 x ChannelMix(\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "    )\n",
       "    (spatial_mix): ModuleList(\n",
       "      (0): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (1-2): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (3): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (4-5): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (6): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (7-8): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (9): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (10-11): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (12): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (13-14): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (15): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (16-17): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (18): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (19-20): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (21): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (22-23): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (24): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (25-26): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (27): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (28-29): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (30): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (31-32): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (33): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (34-35): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (36): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (37-38): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (39): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (40-41): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (42): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (43-44): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (45): SpatialMix(\n",
       "        (grid): [256, 256]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "      (46-47): 2 x SpatialMix(\n",
       "        (grid): [256, 32]\n",
       "        (norm): myLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        )\n",
       "        (scale): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=768, bias=False)\n",
       "        (drop_path): DropPath(prob=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classif): Conv1d(768, 16, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861b00a",
   "metadata": {},
   "source": [
    "## Nuscenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf59aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import utils.transforms as tr\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "class PCDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rootdir=None,\n",
    "        phase=\"train\",\n",
    "        input_feat=\"intensity\",\n",
    "        voxel_size=0.1,\n",
    "        train_augmentations=None,\n",
    "        dim_proj=[\n",
    "            0,\n",
    "        ],\n",
    "        grids_shape=[(256, 256)],\n",
    "        fov_xyz=(\n",
    "            (-1.0, -1.0, -1.0),\n",
    "            (1.0, 1.0, 1.0),\n",
    "        ),\n",
    "        num_neighbors=16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dataset split\n",
    "        self.phase = phase\n",
    "        assert self.phase in [\"train\", \"val\", \"trainval\", \"test\"]\n",
    "\n",
    "        # Root directory of dataset\n",
    "        self.rootdir = rootdir\n",
    "\n",
    "        # Input features to compute for each point\n",
    "        self.input_feat = input_feat\n",
    "\n",
    "        # Downsample input point cloud by small voxelization\n",
    "        self.downsample = tr.Voxelize(\n",
    "            dims=(0, 1, 2),\n",
    "            voxel_size=voxel_size,\n",
    "            random=(self.phase == \"train\" or self.phase == \"trainval\"),\n",
    "        )\n",
    "\n",
    "        # Field of view\n",
    "        assert len(fov_xyz[0]) == len(\n",
    "            fov_xyz[1]\n",
    "        ), \"Min and Max FOV must have the same length.\"\n",
    "        for i, (min, max) in enumerate(zip(*fov_xyz)):\n",
    "            assert (\n",
    "                min < max\n",
    "            ), f\"Field of view: min ({min}) < max ({max}) is expected on dimension {i}.\"\n",
    "        self.fov_xyz = np.concatenate([np.array(f)[None] for f in fov_xyz], axis=0)\n",
    "        self.crop_to_fov = tr.Crop(dims=(0, 1, 2), fov=fov_xyz)\n",
    "\n",
    "        # Grid shape for projection in 2D\n",
    "        assert len(grids_shape) == len(dim_proj)\n",
    "        self.dim_proj = dim_proj\n",
    "        self.grids_shape = [np.array(g) for g in grids_shape]\n",
    "        self.lut_axis_plane = {0: (1, 2), 1: (0, 2), 2: (0, 1)}\n",
    "\n",
    "        # Number of neighbors for embedding layer\n",
    "        assert num_neighbors > 0\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "        # Train time augmentations\n",
    "        if train_augmentations is not None:\n",
    "            assert self.phase in [\"train\", \"trainval\"]\n",
    "        self.train_augmentations = train_augmentations\n",
    "\n",
    "    def get_occupied_2d_cells(self, pc):\n",
    "        \"\"\"Return mapping between 3D point and corresponding 2D cell\"\"\"\n",
    "        cell_ind = []\n",
    "        for dim, grid in zip(self.dim_proj, self.grids_shape):\n",
    "            # Get plane of which to project\n",
    "            dims = self.lut_axis_plane[dim]\n",
    "            # Compute grid resolution\n",
    "            res = (self.fov_xyz[1, dims] - self.fov_xyz[0, dims]) / grid[None]\n",
    "            # Shift and quantize point cloud\n",
    "            pc_quant = ((pc[:, dims] - self.fov_xyz[0, dims]) / res).astype(\"int\")\n",
    "            # Check that the point cloud fits on the grid\n",
    "            min, max = pc_quant.min(0), pc_quant.max(0)\n",
    "            assert min[0] >= 0 and min[1] >= 0, print(\n",
    "                \"Some points are outside the FOV:\", pc[:, :3].min(0), self.fov_xyz\n",
    "            )\n",
    "            assert max[0] < grid[0] and max[1] < grid[1], print(\n",
    "                \"Some points are outside the FOV:\", pc[:, :3].min(0), self.fov_xyz\n",
    "            )\n",
    "            # Transform quantized coordinates to cell indices for projection on 2D plane\n",
    "            temp = pc_quant[:, 0] * grid[1] + pc_quant[:, 1]\n",
    "            cell_ind.append(temp[None])\n",
    "        return np.vstack(cell_ind)\n",
    "\n",
    "    def prepare_input_features(self, pc_orig):\n",
    "        # Concatenate desired input features to coordinates\n",
    "        pc = [pc_orig[:, :3]]  # Initialize with coordinates\n",
    "        for type in self.input_feat:\n",
    "            if type == \"intensity\":\n",
    "                intensity = pc_orig[:, 3:]\n",
    "                intensity = (intensity - self.mean_int) / self.std_int\n",
    "                pc.append(intensity)\n",
    "            elif type == \"height\":\n",
    "                pc.append(pc_orig[:, 2:3])\n",
    "            elif type == \"radius\":\n",
    "                r_xyz = np.linalg.norm(pc_orig[:, :3], axis=1, keepdims=True)\n",
    "                pc.append(r_xyz)\n",
    "            elif type == \"xyz\":\n",
    "                xyz = pc_orig[:, :3]\n",
    "                pc.append(xyz)\n",
    "            elif type == \"constant\":\n",
    "                pc.append(np.ones((pc_orig.shape[0], 1)))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown feature: {type}\")\n",
    "        return np.concatenate(pc, 1)\n",
    "\n",
    "    def load_pc(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load original point cloud\n",
    "        pc_orig, labels_orig, filename = self.load_pc(index)\n",
    "\n",
    "        # Prepare input feature\n",
    "        pc_orig = self.prepare_input_features(pc_orig)\n",
    "\n",
    "        # Voxelization\n",
    "        pc, labels = self.downsample(pc_orig, labels_orig)\n",
    "\n",
    "        # Augment data\n",
    "        if self.train_augmentations is not None:\n",
    "            pc, labels = self.train_augmentations(pc, labels)\n",
    "\n",
    "        # Crop to fov\n",
    "        pc, labels = self.crop_to_fov(pc, labels)\n",
    "\n",
    "        # For each point, get index of corresponding 2D cells on projected grid\n",
    "        cell_ind = self.get_occupied_2d_cells(pc)\n",
    "\n",
    "        # Get neighbors for point embedding layer providing tokens to waffleiron backbone\n",
    "        kdtree = KDTree(pc[:, :3])\n",
    "        assert pc.shape[0] > self.num_neighbors\n",
    "        dist, neighbors_emb = kdtree.query(pc[:, :3], k=self.num_neighbors + 1)\n",
    "\n",
    "        # Nearest neighbor interpolation to undo cropping & voxelisation at validation time\n",
    "        if self.phase in [\"train\", \"trainval\"]:\n",
    "            upsample = np.arange(pc.shape[0])\n",
    "        else:\n",
    "            _, upsample = kdtree.query(pc_orig[:, :3], k=1)\n",
    "\n",
    "        # Output to return\n",
    "        out = (\n",
    "            # Point features\n",
    "            pc[:, 3:].T[None],\n",
    "            # Point labels of original entire point cloud\n",
    "            labels if self.phase in [\"train\", \"trainval\"] else labels_orig,\n",
    "            # Projection 2D -> 3D: index of 2D cells for each point\n",
    "            cell_ind[None],\n",
    "            # Neighborhood for point embedding layer, which provides tokens to waffleiron backbone\n",
    "            neighbors_emb.T[None],\n",
    "            # For interpolation from voxelized & cropped point cloud to original point cloud\n",
    "            upsample,\n",
    "            # Filename of original point cloud\n",
    "            filename,\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df129efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImPcDataset(PCDataset):\n",
    "    \"\"\"\n",
    "    Dataset matching a 3D points cloud and an image using projection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_points, im_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        assert self.phase == \"train\"\n",
    "\n",
    "        self.im_size = im_size\n",
    "\n",
    "        self.limit_num_points = tr.LimitNumPoints(\n",
    "            dims=(0, 1, 2),\n",
    "            max_point=max_points,\n",
    "            random=True,\n",
    "        )\n",
    "\n",
    "        self.pc_augmentations = tr.Compose(\n",
    "            [\n",
    "                tr.Rotation(inplace=True, dim=2),\n",
    "                tr.RandomApply(tr.FlipXY(inplace=True), prob=2 / 3),\n",
    "                tr.Scale(inplace=True, dims=(0, 1, 2), range=0.1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def load_pc(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def map_pc_to_image(self, pc, index, min_dist=1.0):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def resize_im(self, im, pairing_images):\n",
    "        # Rescale pixel coordinates\n",
    "        rescale = [1.0, self.im_size[0] / im.shape[-2], self.im_size[1] / im.shape[-1]]\n",
    "        pairing_images = np.floor(np.multiply(pairing_images, rescale))\n",
    "        pairing_images = pairing_images.astype(np.int64)\n",
    "        # Rescale image\n",
    "        im = vision_resize(im, self.im_size)\n",
    "        return im, pairing_images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load original point cloud\n",
    "        pc = self.load_pc(index)\n",
    "\n",
    "        # Voxelization\n",
    "        pc, _ = self.downsample(pc, None)\n",
    "\n",
    "        # Project point cloud to image\n",
    "        pc, images, pairing_images = self.map_pc_to_image(pc, index)\n",
    "        images = torch.tensor(np.array(images, dtype=np.float32).transpose(0, 3, 1, 2))\n",
    "        assert len(pairing_images) > 0\n",
    "\n",
    "        # Limit number of points and ...\n",
    "        pc, _, idx = self.limit_num_points(pc, None, return_idx=True)\n",
    "        # ... adapt (points, pixels) pairs\n",
    "        pairing_images = pairing_images[idx]\n",
    "\n",
    "        # Apply augmentations\n",
    "        pc, _ = self.pc_augmentations(pc, None)\n",
    "        images, pairing_images = self.resize_im(images, pairing_images)\n",
    "\n",
    "        # Crop to fov and ...\n",
    "        pc, _, where = self.crop_to_fov(pc, None, return_mask=True)\n",
    "        # ... adapt (points, pixels) pairs\n",
    "        pairing_images = pairing_images[where]\n",
    "\n",
    "        # Get point features\n",
    "        pc = self.prepare_input_features(pc)\n",
    "\n",
    "        # Projection on 2D grid\n",
    "        cell_ind = self.get_occupied_2d_cells(pc)\n",
    "\n",
    "        # Embedding\n",
    "        kdtree = KDTree(pc[:, :3])\n",
    "        assert pc.shape[0] > self.num_neighbors\n",
    "        _, neighbors = kdtree.query(pc[:, :3], k=self.num_neighbors + 1)\n",
    "\n",
    "        out = (\n",
    "            pc[:, 3:].T[None],\n",
    "            images,\n",
    "            np.arange(pc.shape[0]),\n",
    "            pairing_images,\n",
    "            cell_ind[None],\n",
    "            neighbors.T[None],\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c282a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 - Valeo Comfort and Driving Assistance - valeo.ai\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from nuscenes import NuScenes\n",
    "\n",
    "# For normalizing intensities\n",
    "MEAN_INT = 18.742355\n",
    "STD_INT = 22.04632\n",
    "\n",
    "\n",
    "class ClassMapper:\n",
    "    def __init__(self):\n",
    "        current_folder = os.path.dirname('/home/HyperLiDAR/datasets/')\n",
    "        self.mapping = np.load(\n",
    "            os.path.join(current_folder, \"mapping_class_index_nuscenes.npy\")\n",
    "        )\n",
    "\n",
    "    def get_index(self, x):\n",
    "        if x < len(self.mapping):\n",
    "            if (self.mapping[x]) == 255:\n",
    "                print(255)\n",
    "            return self.mapping[x]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "class NuScenesSemSeg(PCDataset):\n",
    "\n",
    "    CLASS_NAME = [\n",
    "        \"barrier\",\n",
    "        \"bicycle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"construction_vehicle\",\n",
    "        \"motorcycle\",\n",
    "        \"pedestrian\",\n",
    "        \"traffic_cone\",\n",
    "        \"trailer\",\n",
    "        \"truck\",\n",
    "        \"driveable_surface\",\n",
    "        \"other_flat\",\n",
    "        \"sidewalk\",\n",
    "        \"terrain\",\n",
    "        \"manmade\",\n",
    "        \"vegetation\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, ratio=\"100p\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # For normalizing intensities\n",
    "        self.mean_int = MEAN_INT\n",
    "        self.std_int = STD_INT\n",
    "\n",
    "        # Class mapping\n",
    "        current_folder = os.path.dirname('/home/HyperLiDAR/datasets/')\n",
    "        self.mapper = np.vectorize(ClassMapper().get_index)\n",
    "\n",
    "        # List all keyframes\n",
    "        self.ratio = ratio\n",
    "        if self.phase == \"train\":\n",
    "            if self.ratio == \"100p\":\n",
    "                self.list_frames = np.load(\n",
    "                    os.path.join(current_folder, \"list_files_nuscenes.npz\")\n",
    "                )[self.phase]\n",
    "            elif self.ratio == \"10p\":\n",
    "                self.list_frames = np.load(\n",
    "                    os.path.join(current_folder, \"nuscenes-ratio_10-v_0.npy\"),\n",
    "                    allow_pickle=True,\n",
    "                )\n",
    "            elif self.ratio == \"1p\":\n",
    "                self.list_frames = np.load(\n",
    "                    os.path.join(current_folder, \"nuscenes-ratio_100-v_0.npy\"),\n",
    "                    allow_pickle=True,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unprepared nuScenes split {self.ratio}.\")\n",
    "        \n",
    "        elif self.phase == \"val\":\n",
    "            nusc = NuScenes(version='v1.0-mini', dataroot=kwargs['rootdir'], verbose=True)\n",
    "            #self.list_frames = np.load(\n",
    "            #    os.path.join(current_folder, \"list_files_nuscenes.npz\")\n",
    "            #)[self.phase]\n",
    "            array = []\n",
    "            for i in range(len(nusc.sample)):\n",
    "                base = nusc.sample[i]\n",
    "                for j, f in enumerate(os.listdir('/mnt/data/dataset/nuscenes/samples/LIDAR_TOP')):\n",
    "                    if f[42:-8] == str(base['timestamp']):\n",
    "                        break\n",
    "                sample = 'samples/LIDAR_TOP/' + f\n",
    "                lidarseg = 'lidarseg/v1.0-mini/' + base['data']['LIDAR_TOP'] + '_lidarseg.bin'\n",
    "                token = base['data']['LIDAR_TOP']\n",
    "                array.append([sample, lidarseg, token])\n",
    "            self.list_frames = array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_frames)\n",
    "\n",
    "    def load_pc(self, index):\n",
    "        # Load point cloud\n",
    "        pc = np.fromfile(\n",
    "            os.path.join(self.rootdir, self.list_frames[index][0]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        pc = pc.reshape((-1, 5))[:, :4]\n",
    "\n",
    "        # Load segmentation labels\n",
    "        labels = np.fromfile(\n",
    "            os.path.join(self.rootdir, self.list_frames[index][1]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        labels = self.mapper(labels)\n",
    "\n",
    "        # Label 0 should be ignored\n",
    "        labels = labels - 1\n",
    "        labels[labels == -1] = 255 # Here\n",
    "\n",
    "        return pc, labels, self.list_frames[index][2]\n",
    "\n",
    "\n",
    "class NuScenesDistill(ImPcDataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # For normalizing intensities\n",
    "        self.mean_int = MEAN_INT\n",
    "        self.std_int = STD_INT\n",
    "\n",
    "        # List of available cameras\n",
    "        self.camera_list = [\n",
    "            \"CAM_FRONT\",\n",
    "            \"CAM_FRONT_RIGHT\",\n",
    "            \"CAM_BACK_RIGHT\",\n",
    "            \"CAM_BACK\",\n",
    "            \"CAM_BACK_LEFT\",\n",
    "            \"CAM_FRONT_LEFT\",\n",
    "        ]\n",
    "\n",
    "        # Load data\n",
    "        self.list_keyframes = np.load(\n",
    "            os.path.join(\n",
    "                os.path.dirname(os.path.realpath(__file__)),\n",
    "                f\"nuscenes_data_{self.phase}.npy\",\n",
    "            ),\n",
    "            allow_pickle=True,\n",
    "        ).item()\n",
    "        assert len(self.list_keyframes) == 28130\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_keyframes)\n",
    "\n",
    "    def load_pc(self, index):\n",
    "        pc = np.fromfile(\n",
    "            os.path.join(\n",
    "                self.rootdir,\n",
    "                self.list_keyframes[index][\"point\"][\"filename\"],\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        return pc.reshape((-1, 5))[:, :4]\n",
    "\n",
    "    def map_pc_to_image(self, pc, index, min_dist=1.0):\n",
    "        # Make point cloud compatible with nuscenes-devkit format\n",
    "        pc_base = LidarPointCloud(pc.T)\n",
    "\n",
    "        # Choose one camera\n",
    "        camera_name = self.camera_list[torch.randint(len(self.camera_list), (1,))[0]]\n",
    "\n",
    "        # Load image\n",
    "        im = np.array(\n",
    "            Image.open(\n",
    "                os.path.join(\n",
    "                    self.rootdir, self.list_keyframes[index][camera_name][\"filename\"]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "        pc_copy = copy.deepcopy(pc_base)\n",
    "        cs_record = self.list_keyframes[index][\"point\"][\"cs_record\"]\n",
    "        pc_copy.rotate(Quaternion(cs_record[\"rotation\"]).rotation_matrix)\n",
    "        pc_copy.translate(np.array(cs_record[\"translation\"]))\n",
    "\n",
    "        # Second step: transform from ego to the global frame.\n",
    "        poserecord = self.list_keyframes[index][\"point\"][\"poserecord\"]\n",
    "        pc_copy.rotate(Quaternion(poserecord[\"rotation\"]).rotation_matrix)\n",
    "        pc_copy.translate(np.array(poserecord[\"translation\"]))\n",
    "\n",
    "        # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "        poserecord = self.list_keyframes[index][camera_name][\"poserecord\"]\n",
    "        pc_copy.translate(-np.array(poserecord[\"translation\"]))\n",
    "        pc_copy.rotate(Quaternion(poserecord[\"rotation\"]).rotation_matrix.T)\n",
    "\n",
    "        # Fourth step: transform from ego into the camera.\n",
    "        cs_record = self.list_keyframes[index][camera_name][\"cs_record\"]\n",
    "        pc_copy.translate(-np.array(cs_record[\"translation\"]))\n",
    "        pc_copy.rotate(Quaternion(cs_record[\"rotation\"]).rotation_matrix.T)\n",
    "\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pc_copy.points[2, :]\n",
    "\n",
    "        # Take a \"picture\" of the point cloud\n",
    "        # (matrix multiplication with camera-matrix + renormalization).\n",
    "        projected_points = view_points(\n",
    "            pc_copy.points[:3, :],\n",
    "            np.array(cs_record[\"camera_intrinsic\"]),\n",
    "            normalize=True,\n",
    "        )\n",
    "\n",
    "        # Remove points that are either outside or behind the camera.\n",
    "        # Also make sure points are at least 1m in front of the camera\n",
    "        projected_points = projected_points[:2].T\n",
    "        mask = np.ones(depths.shape[0], dtype=bool)\n",
    "        mask = np.logical_and(mask, depths > min_dist)\n",
    "        mask = np.logical_and(mask, projected_points[:, 0] > 0)\n",
    "        mask = np.logical_and(mask, projected_points[:, 0] < im.shape[1] - 1)\n",
    "        mask = np.logical_and(mask, projected_points[:, 1] > 0)\n",
    "        mask = np.logical_and(mask, projected_points[:, 1] < im.shape[0] - 1)\n",
    "\n",
    "        # Apply the mask\n",
    "        projected_points = projected_points[mask]\n",
    "        pc_base.points = pc_base.points[:, mask]\n",
    "\n",
    "        # For points with a matching pixel, coordinates of that pixel (size N x 2)\n",
    "        # Use flip for change from (x, y) to (row, column)\n",
    "        matching_pixels = np.floor(np.flip(projected_points, axis=1)).astype(np.int64)\n",
    "\n",
    "        # Append data\n",
    "        images = [im / 255.0]\n",
    "        matching_pixels = np.concatenate(\n",
    "            (\n",
    "                np.zeros((matching_pixels.shape[0], 1), dtype=np.int64),\n",
    "                matching_pixels,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        pairing_images = [matching_pixels]\n",
    "\n",
    "        return pc_base.points.T, images, np.concatenate(pairing_images)\n",
    "\n",
    "\n",
    "class NuScenesSemSeg_1p(NuScenesSemSeg):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(ratio=\"1p\", **kwargs)\n",
    "\n",
    "\n",
    "class NuScenesSemSeg_10p(NuScenesSemSeg):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(ratio=\"10p\", **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337afe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, num_points=None):\n",
    "        self.num_points = num_points\n",
    "        assert num_points is None or num_points > 0\n",
    "\n",
    "    def __call__(self, list_data):\n",
    "\n",
    "        # Extract all data\n",
    "        list_of_data = (list(data) for data in zip(*list_data))\n",
    "        feat, label_orig, cell_ind, neighbors_emb, upsample, filename = list_of_data\n",
    "\n",
    "        # Zero-pad point clouds\n",
    "        Nmax = np.max([f.shape[-1] for f in feat])\n",
    "        if self.num_points is not None:\n",
    "            assert Nmax <= self.num_points\n",
    "        occupied_cells = []\n",
    "        for i in range(len(feat)):\n",
    "            feat[i], neighbors_emb[i], cell_ind[i], temp = zero_pad(\n",
    "                feat[i],\n",
    "                neighbors_emb[i],\n",
    "                cell_ind[i],\n",
    "                Nmax if self.num_points is None else self.num_points,\n",
    "            )\n",
    "            occupied_cells.append(temp)\n",
    "\n",
    "        # Concatenate along batch dimension\n",
    "        feat = torch.from_numpy(np.vstack(feat)).float()  # B x C x Nmax\n",
    "        neighbors_emb = torch.from_numpy(np.vstack(neighbors_emb)).long()  # B x Nmax\n",
    "        cell_ind = torch.from_numpy(\n",
    "            np.vstack(cell_ind)\n",
    "        ).long()  # B x nb_2d_cells x Nmax\n",
    "        occupied_cells = torch.from_numpy(np.vstack(occupied_cells)).float()  # B x Nmax\n",
    "        labels_orig = torch.from_numpy(np.hstack(label_orig)).long()\n",
    "        upsample = [torch.from_numpy(u) for u in upsample]\n",
    "\n",
    "        # Prepare output variables\n",
    "        out = {\n",
    "            \"feat\": feat,\n",
    "            \"neighbors_emb\": neighbors_emb,\n",
    "            \"upsample\": upsample,\n",
    "            \"labels_orig\": labels_orig,\n",
    "            \"cell_ind\": cell_ind,\n",
    "            \"occupied_cells\": occupied_cells,\n",
    "            \"filename\": filename,\n",
    "        }\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1814ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(feat, neighbors_emb, cell_ind, Nmax):\n",
    "    N = feat.shape[-1]\n",
    "    assert N <= Nmax\n",
    "    occupied_cells = np.ones((1, Nmax))\n",
    "    if N < Nmax:\n",
    "        # Zero-pad with null features\n",
    "        feat = np.concatenate((feat, np.zeros((1, feat.shape[1], Nmax - N))), axis=2)\n",
    "        # For zero-padded points, associate last zero-padded points as neighbor\n",
    "        neighbors_emb = np.concatenate(\n",
    "            (\n",
    "                neighbors_emb,\n",
    "                (Nmax - 1) * np.ones((1, neighbors_emb.shape[1], Nmax - N)),\n",
    "            ),\n",
    "            axis=2,\n",
    "        )\n",
    "        # Associate zero-padded points to first 2D cell...\n",
    "        cell_ind = np.concatenate(\n",
    "            (cell_ind, np.zeros((1, cell_ind.shape[1], Nmax - N))), axis=2\n",
    "        )\n",
    "        # ... and at the same time mark zero-padded points as unoccupied\n",
    "        occupied_cells[:, N:] = 0\n",
    "    return feat, neighbors_emb, cell_ind, occupied_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3bad12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.574 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "        \"rootdir\": '/root/main/dataset/nuscenes',\n",
    "        \"input_feat\": [\"xyz\", \"intensity\", \"radius\"],\n",
    "        \"voxel_size\": 0.1,\n",
    "        \"num_neighbors\": 16,\n",
    "        \"dim_proj\": [2, 1, 0],\n",
    "        \"grids_shape\": [[256, 256], [256, 32], [256, 32]],\n",
    "        \"fov_xyz\": [[-64, -64, -8], [64, 64, 8]], # Check here\n",
    "    }\n",
    "\n",
    "train_dataset = NuScenesSemSeg(\n",
    "        phase=\"val\",\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b5217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1dae585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=Collate(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ce92bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768, 16092])\n",
      "torch.Size([2, 1, 768, 16092])\n",
      "torch.Size([2, 1, 768, 16092])\n",
      "torch.Size([3, 1, 768, 16092])\n",
      "torch.Size([3, 1, 768, 16092])\n",
      "torch.Size([4, 1, 768, 16092])\n",
      "torch.Size([4, 1, 768, 16092])\n",
      "torch.Size([5, 1, 768, 16092])\n",
      "torch.Size([5, 1, 768, 16092])\n",
      "torch.Size([6, 1, 768, 16092])\n",
      "torch.Size([6, 1, 768, 16092])\n",
      "torch.Size([7, 1, 768, 16092])\n",
      "torch.Size([7, 1, 768, 16092])\n",
      "torch.Size([8, 1, 768, 16092])\n",
      "torch.Size([8, 1, 768, 16092])\n",
      "torch.Size([9, 1, 768, 16092])\n",
      "torch.Size([9, 1, 768, 16092])\n",
      "torch.Size([10, 1, 768, 16092])\n",
      "torch.Size([10, 1, 768, 16092])\n",
      "torch.Size([11, 1, 768, 16092])\n",
      "torch.Size([11, 1, 768, 16092])\n",
      "torch.Size([12, 1, 768, 16092])\n",
      "torch.Size([12, 1, 768, 16092])\n",
      "torch.Size([13, 1, 768, 16092])\n",
      "torch.Size([13, 1, 768, 16092])\n",
      "torch.Size([14, 1, 768, 16092])\n",
      "torch.Size([14, 1, 768, 16092])\n",
      "torch.Size([15, 1, 768, 16092])\n",
      "torch.Size([15, 1, 768, 16092])\n",
      "torch.Size([16, 1, 768, 16092])\n",
      "torch.Size([16, 1, 768, 16092])\n",
      "torch.Size([17, 1, 768, 16092])\n",
      "torch.Size([17, 1, 768, 16092])\n",
      "torch.Size([18, 1, 768, 16092])\n",
      "torch.Size([18, 1, 768, 16092])\n",
      "torch.Size([19, 1, 768, 16092])\n",
      "torch.Size([19, 1, 768, 16092])\n",
      "torch.Size([20, 1, 768, 16092])\n",
      "torch.Size([20, 1, 768, 16092])\n",
      "torch.Size([21, 1, 768, 16092])\n",
      "torch.Size([21, 1, 768, 16092])\n",
      "torch.Size([22, 1, 768, 16092])\n",
      "torch.Size([22, 1, 768, 16092])\n",
      "torch.Size([23, 1, 768, 16092])\n",
      "torch.Size([23, 1, 768, 16092])\n",
      "torch.Size([24, 1, 768, 16092])\n",
      "torch.Size([24, 1, 768, 16092])\n",
      "torch.Size([25, 1, 768, 16092])\n",
      "torch.Size([25, 1, 768, 16092])\n",
      "torch.Size([26, 1, 768, 16092])\n",
      "torch.Size([26, 1, 768, 16092])\n",
      "torch.Size([27, 1, 768, 16092])\n",
      "torch.Size([27, 1, 768, 16092])\n",
      "torch.Size([28, 1, 768, 16092])\n",
      "torch.Size([28, 1, 768, 16092])\n",
      "torch.Size([29, 1, 768, 16092])\n",
      "torch.Size([29, 1, 768, 16092])\n",
      "torch.Size([30, 1, 768, 16092])\n",
      "torch.Size([30, 1, 768, 16092])\n",
      "torch.Size([31, 1, 768, 16092])\n",
      "torch.Size([31, 1, 768, 16092])\n",
      "torch.Size([32, 1, 768, 16092])\n",
      "torch.Size([32, 1, 768, 16092])\n",
      "torch.Size([33, 1, 768, 16092])\n",
      "torch.Size([33, 1, 768, 16092])\n",
      "torch.Size([34, 1, 768, 16092])\n",
      "torch.Size([34, 1, 768, 16092])\n",
      "torch.Size([35, 1, 768, 16092])\n",
      "torch.Size([35, 1, 768, 16092])\n",
      "torch.Size([36, 1, 768, 16092])\n",
      "torch.Size([36, 1, 768, 16092])\n",
      "torch.Size([37, 1, 768, 16092])\n",
      "torch.Size([37, 1, 768, 16092])\n",
      "torch.Size([38, 1, 768, 16092])\n",
      "torch.Size([38, 1, 768, 16092])\n",
      "torch.Size([39, 1, 768, 16092])\n",
      "torch.Size([39, 1, 768, 16092])\n",
      "torch.Size([40, 1, 768, 16092])\n",
      "torch.Size([40, 1, 768, 16092])\n",
      "torch.Size([41, 1, 768, 16092])\n",
      "torch.Size([41, 1, 768, 16092])\n",
      "torch.Size([42, 1, 768, 16092])\n",
      "torch.Size([42, 1, 768, 16092])\n",
      "torch.Size([43, 1, 768, 16092])\n",
      "torch.Size([43, 1, 768, 16092])\n",
      "torch.Size([44, 1, 768, 16092])\n",
      "torch.Size([44, 1, 768, 16092])\n",
      "torch.Size([45, 1, 768, 16092])\n",
      "torch.Size([45, 1, 768, 16092])\n",
      "torch.Size([46, 1, 768, 16092])\n",
      "torch.Size([46, 1, 768, 16092])\n",
      "torch.Size([47, 1, 768, 16092])\n",
      "torch.Size([47, 1, 768, 16092])\n",
      "torch.Size([48, 1, 768, 16092])\n",
      "torch.Size([48, 1, 768, 16092])\n",
      "torch.Size([49, 1, 768, 16092])\n"
     ]
    }
   ],
   "source": [
    "for it, batch in enumerate(train_loader):\n",
    "    \n",
    "    if it == 15: # Only the first sample\n",
    "\n",
    "        # Network inputs\n",
    "        #print(batch[\"upsample\"])\n",
    "        feat = batch[\"feat\"].cuda(0, non_blocking=True)\n",
    "        labels = batch[\"labels_orig\"].cuda(0, non_blocking=True)\n",
    "        batch[\"upsample\"] = [\n",
    "            up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "        ]\n",
    "        cell_ind = batch[\"cell_ind\"].cuda(0, non_blocking=True)\n",
    "        occupied_cell = batch[\"occupied_cells\"].cuda(0, non_blocking=True)\n",
    "        neighbors_emb = batch[\"neighbors_emb\"].cuda(0, non_blocking=True)\n",
    "        net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(*net_inputs)\n",
    "        \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f2e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed, tokens, out = out[0], out[1], out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6999c981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34784])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f0ca339-bb21-4419-8e3f-adb57708e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 16092])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "217de35f-77c5-4f25-95ba-cdad8b1647c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 1, 768, 16092])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape ### All the intermediate from embed to the final layer which we pass to HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3350fe7c-8e8e-4c2a-835e-86a0dd1f2ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16092])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2644ef1e-1f7d-4d33-8e17-36ae00258517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(embed, tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764bc75-06a0-4028-9729-d44920e082bc",
   "metadata": {},
   "source": [
    "# HD algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
