{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33f8a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.scatter_reduce for 3D to 2D projection.\n",
      "Using torch.scatter_reduce for 3D to 2D projection.\n"
     ]
    }
   ],
   "source": [
    "from models.waffleiron.segmenter import Segmenter\n",
    "import torch\n",
    "from datasets import LIST_DATASETS, Collate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torchhd\n",
    "from torchhd.models import Centroid\n",
    "from torchhd import embeddings\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hd_dim, size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.projection = embeddings.Projection(size, hd_dim)\n",
    "        self.projection.weight = nn.Parameter(torchhd.normalize(self.projection.weight), requires_grad=False) # Binary\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample_hv = self.projection(x)\n",
    "        return torchhd.hard_quantize(sample_hv)\n",
    "\n",
    "class Feature_Extractor:\n",
    "    def __init__(self, input_channels=5, feat_channels=768, depth=48, \n",
    "                 grid_shape=[[256, 256], [256, 32], [256, 32]], nb_class=16, layer_norm=True, \n",
    "                 device=torch.device(\"cpu\"), early_exit = [48], **kwargs):\n",
    "        self.model = Segmenter(\n",
    "            input_channels=input_channels,\n",
    "            feat_channels=feat_channels,\n",
    "            depth=depth,\n",
    "            grid_shape=grid_shape,\n",
    "            nb_class=nb_class, # class for prediction\n",
    "            #drop_path_prob=config[\"waffleiron\"][\"drop_path\"],\n",
    "            layer_norm=layer_norm,\n",
    "            early_exit = early_exit\n",
    "        )\n",
    "\n",
    "        classif = torch.nn.Conv1d(\n",
    "            feat_channels, nb_class, 1 # So it fits 16 = nb_class but classifier is not used\n",
    "        )\n",
    "        torch.nn.init.constant_(classif.bias, 0)\n",
    "        torch.nn.init.constant_(classif.weight, 0)\n",
    "        self.model.classif = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(feat_channels),\n",
    "            classif,\n",
    "        )\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.model.classif.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        def get_optimizer(parameters):\n",
    "            return torch.optim.AdamW(\n",
    "                parameters,\n",
    "                lr=0.001,\n",
    "                weight_decay=0.003,\n",
    "            )\n",
    "\n",
    "        optim = get_optimizer(self.model.parameters())\n",
    "        self.device = device\n",
    "        self.device_string = \"cuda:0\" if (torch.cuda.is_available() and kwargs['args'].device == 'gpu') else \"cpu\"\n",
    "        self.num_classes = nb_class\n",
    "        self.early_exit = early_exit\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def load_pretrained(self, path):\n",
    "        # Load pretrained model\n",
    "        path_to_ckpt = path\n",
    "        checkpoint = torch.load(path_to_ckpt,\n",
    "            map_location=self.device_string)\n",
    "        state_dict = checkpoint[\"net\"]  # Adjust key as needed\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            new_key = k.replace(\"module.\", \"\")  # Remove \"module.\" prefix\n",
    "            new_state_dict[new_key] = v\n",
    "\n",
    "        self.model.load_state_dict(new_state_dict)\n",
    "\n",
    "        print(\n",
    "            f\"Checkpoint loaded on {self.device_string}: {path_to_ckpt}\"\n",
    "        )\n",
    "\n",
    "        if self.device_string != 'cpu':\n",
    "            torch.cuda.set_device(self.device_string) # cuda:0\n",
    "            self.model = self.model.cuda(self.device_string) # cuda:0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        #self.model.waffleiron.crop_model(self.early_exit)\n",
    "\n",
    "    def forward_model(self, it, batch):\n",
    "\n",
    "        # Checking all of the parameters needed for feature extractor\n",
    "        # Obj: only pass what you need\n",
    "        feat = batch[\"feat\"]\n",
    "        labels = batch[\"labels_orig\"]\n",
    "        cell_ind = batch[\"cell_ind\"]\n",
    "        occupied_cell = batch[\"occupied_cells\"]\n",
    "        neighbors_emb = batch[\"neighbors_emb\"]\n",
    "        if self.device_string != 'cpu':\n",
    "            feat = feat.cuda(0, non_blocking=True)\n",
    "            labels = labels.cuda(0, non_blocking=True)\n",
    "            batch[\"upsample\"] = [\n",
    "                up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "            ]\n",
    "            cell_ind = cell_ind.cuda(0, non_blocking=True)\n",
    "            occupied_cell = occupied_cell.cuda(0, non_blocking=True)\n",
    "            neighbors_emb = neighbors_emb.cuda(0, non_blocking=True)\n",
    "        net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "        if self.device_string != 'cpu':\n",
    "            with torch.autocast(\"cuda\", enabled=True):\n",
    "                # Logits\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(*net_inputs)\n",
    "                    encode, tokens, out, exit_layer = out[0], out[1], out[2], out[3]\n",
    "                    pred_label = out.max(1)[1]\n",
    "\n",
    "                    # Only return samples that are not noise\n",
    "                    #torch.cuda.synchronize(device=self.device)\n",
    "                    where = labels != 255\n",
    "                    #torch.cuda.synchronize(device=self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.model(*net_inputs)\n",
    "                encode, tokens, out, exit_layer = out[0], out[1], out[2], out[3]\n",
    "                pred_label = out.max(1)[1]\n",
    "\n",
    "                # Only return samples that are not noise\n",
    "                where = labels != 255\n",
    "\n",
    "        return tokens[0,:,where], labels[where], pred_label[0, where], exit_layer\n",
    "\n",
    "    def test(self, loader, total_voxels):        \n",
    "        # Metric\n",
    "        miou = MulticlassJaccardIndex(num_classes=self.num_classes, average=None).to(self.device, non_blocking=True)\n",
    "        final_labels = torch.empty((total_voxels), device=self.device)\n",
    "        final_pred = torch.empty((total_voxels), device=self.device)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for it, batch in tqdm(enumerate(loader), desc=\"SoA testing\"):\n",
    "            features, labels, soa_result = self.forward_model(it, batch)\n",
    "            shape_sample = labels.shape[0]\n",
    "            labels = labels.to(dtype = torch.int64, device = self.device, non_blocking=True)\n",
    "            soa_result = soa_result.to(device=self.device, non_blocking=True)\n",
    "            final_labels[start_idx:start_idx+shape_sample] = labels\n",
    "\n",
    "            final_pred[start_idx:start_idx+shape_sample] = soa_result\n",
    "\n",
    "            start_idx += shape_sample\n",
    "\n",
    "        final_labels = final_labels[:start_idx]\n",
    "        final_pred = final_pred[:start_idx]\n",
    "\n",
    "        print(\"================================\")\n",
    "\n",
    "        print('Pred FE', final_pred, \"\\tShape: \", final_pred.shape)\n",
    "        print('Label', final_labels, \"\\tShape: \", final_labels.shape)\n",
    "        accuracy = miou(final_pred, final_labels)\n",
    "        avg_acc = torch.mean(accuracy)\n",
    "        print(f'accuracy: {accuracy}')\n",
    "        print(f'avg acc: {avg_acc}')\n",
    "\n",
    "        #cm = confusion_matrix(pred_hd, first_label, labels=torch.Tensor(range(0,15)))\n",
    "        #print(\"Confusion matrix \\n\")\n",
    "        #print(cm)\n",
    "\n",
    "        print(\"================================\")\n",
    "\n",
    "class HD_Model:\n",
    "    def __init__(self, in_dim, out_dim, num_classes, path_pretrained, \n",
    "                 device=torch.device(\"cpu\"), **kwargs):\n",
    "\n",
    "        encode = Encoder(out_dim, in_dim)\n",
    "        self.encode = encode.to(device=device, non_blocking=True)\n",
    "\n",
    "        model = Centroid(out_dim, num_classes)\n",
    "        self.model = model.to(device=device, non_blocking=True)\n",
    "        self.device = device\n",
    "        self.feature_extractor = Feature_Extractor(nb_class = num_classes, device=self.device, \n",
    "                                                   early_exit=[int(i) for i in kwargs['args'].layers], \n",
    "                                                   args=kwargs['args'])\n",
    "        self.feature_extractor.load_pretrained(path_pretrained)\n",
    "        self.stop = kwargs['args'].layers\n",
    "        self.point_per_iter = kwargs['args'].number_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.max_samples = kwargs['args'].number_samples\n",
    "        self.kwargs = kwargs\n",
    "        self.compensation = None\n",
    "\n",
    "    def normalize(self, samples):\n",
    "\n",
    "        \"\"\" Normalize with Z-score\"\"\"\n",
    "\n",
    "        mean = torch.mean(samples, dim=0)\n",
    "        std = torch.std(samples, dim=0)\n",
    "\n",
    "        samples = (samples - mean) / (std + 1e-8)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader):\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.num_vox_train = 0\n",
    "        self.num_vox_val = 0\n",
    "\n",
    "        for loader, desc, attr in [(self.train_loader, \"Training loader\", \"num_vox_train\"),\n",
    "                           (self.val_loader, \"Validation loader\", \"num_vox_val\")]:\n",
    "            for batch in tqdm(loader, desc=desc):\n",
    "                labels = batch[\"labels_orig\"]\n",
    "\n",
    "                # Ensure labels are tensors\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.stack(labels)  # Convert list of tensors to a single tensor\n",
    "                \n",
    "                # Move to GPU if applicable\n",
    "                if self.device.type == \"cuda\":\n",
    "                    labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "                # Compute the number of valid voxels\n",
    "                setattr(self, attr, getattr(self, attr) + (labels != 255).sum().item())\n",
    "\n",
    "        print(\"Finished loading data loaders\")\n",
    "\n",
    "    def set_compensation(self, inter_weights_path):\n",
    "\n",
    "        \"\"\"Load all the paths for every exit\"\"\"\n",
    "\n",
    "        self.linear_weights = {}\n",
    "\n",
    "        for layer, path in inter_weights_path.items():\n",
    "            self.linear_weights[layer] = nn.Linear(768, 768)\n",
    "            state_dict = torch.load(path)\n",
    "            self.linear_weights[layer].load_state_dict(state_dict)\n",
    "            self.linear_weights[layer] = self.linear_weights[layer].to(self.device)\n",
    "        self.compensation = True\n",
    "    \n",
    "    def sample_to_encode(self, it, batch):\n",
    "        features, labels, soa_result, exit_layer = self.feature_extractor.forward_model(it, batch) # Everything for what hasn't been dropped\n",
    "        features = torch.transpose(features, 0, 1).to(dtype=torch.float32, device = self.device, non_blocking=True)\n",
    "        labels = labels.to(dtype=torch.int64, device = self.device, non_blocking=True)\n",
    "\n",
    "        if self.compensation and exit_layer != 47:\n",
    "            features = self.linear_weights[exit_layer](features)\n",
    "\n",
    "        features = self.normalize(features) # Z1 score seems to work\n",
    "\n",
    "        # HD training\n",
    "        samples_hv = self.encode(features)\n",
    "\n",
    "        return samples_hv, labels\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        \"\"\" Initial training pass \"\"\"\n",
    "\n",
    "        print(\"\\nTrain First\\n\")\n",
    "\n",
    "        for it, batch in tqdm(enumerate(self.train_loader), desc=\"Training\"):\n",
    " \n",
    "            samples_hv, labels = self.sample_to_encode(it, batch)\n",
    "            \n",
    "            for b in range(0, samples_hv.shape[0], self.point_per_iter):\n",
    "                end = min(b + self.point_per_iter, int(samples_hv.shape[0]))  # Ensure we don't exceed num_voxels[i]\n",
    "        \n",
    "            #samples_hv = samples_hv.reshape((1,samples_hv.shape[0]))\n",
    "            \n",
    "                self.model.add(samples_hv[b:end], labels[b:end])\n",
    "\n",
    "                #if self.device == torch.device(\"cuda:0\"):\n",
    "                #    torch.cuda.synchronize(device=self.device)\n",
    "            if it == self.max_samples:\n",
    "                break\n",
    "            \n",
    "        self.model.weight = nn.Parameter(torchhd.normalize(self.model.weight), requires_grad=False) # Binary\n",
    "\n",
    "    def retrain(self, epochs):\n",
    "        \n",
    "        \"\"\" Retrain with misclassified samples (also substract)\"\"\"\n",
    "        \n",
    "        for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "            #count = 0\n",
    "            #self.scramble = np.random.permutation(len(self.im_idx))\n",
    "\n",
    "            for it, batch in tqdm(enumerate(self.train_loader), desc=f\"Retraining epoch {e}\"):\n",
    "                \n",
    "                samples_hv, labels = self.sample_to_encode(it, batch)\n",
    "                for b in range(0, samples_hv.shape[0], self.point_per_iter):\n",
    "                    end = min(b + self.point_per_iter, int(samples_hv.shape[0]))  # Ensure we don't exceed num_voxels[i]\n",
    "                    samples_hv_here = samples_hv[b:end]\n",
    "                    labels_here = labels[b:end]\n",
    "                    sim = self.model(samples_hv_here, dot=True)\n",
    "                    #pred_hd = sim.argmax(1).data\n",
    "                    pred_hd = torch.argmax(sim, axis=1)\n",
    "\n",
    "                    is_wrong = labels_here != pred_hd\n",
    "\n",
    "                    # cancel update if all predictions were correct\n",
    "                    if is_wrong.sum().item() == 0:\n",
    "                        continue\n",
    "\n",
    "                    # only update wrongly predicted inputs\n",
    "                    samples_hv_here = samples_hv_here[is_wrong]\n",
    "                    labels_here = labels_here[is_wrong]\n",
    "                    pred_hd = pred_hd[is_wrong]\n",
    "\n",
    "                    #count = labels.shape[0]\n",
    "\n",
    "                    self.model.weight.index_add_(0, labels_here, samples_hv_here)\n",
    "                    self.model.weight.index_add_(0, pred_hd, samples_hv_here, alpha=-1.0)\n",
    "\n",
    "                #torch.cuda.synchronize(device=self.device)\n",
    "\n",
    "                if it == self.max_samples:\n",
    "                    break\n",
    "\n",
    "            #print(f\"Misclassified for {it}: \", count)\n",
    "\n",
    "            # If you want to test for each sample\n",
    "            self.test_hd()\n",
    "\n",
    "    def test_hd(self, loader='val'):\n",
    "\n",
    "        \"\"\" Testing over all the samples in all the scans given \"\"\"\n",
    "\n",
    "        if loader == 'val':\n",
    "            loader = self.val_loader\n",
    "            num_vox = self.num_vox_val\n",
    "        else:\n",
    "            loader = self.train_loader\n",
    "            num_vox = self.num_vox_train\n",
    "        \n",
    "        # Metric\n",
    "        miou = MulticlassJaccardIndex(num_classes=self.num_classes, average=None).to(self.device, non_blocking=True)\n",
    "        final_labels = torch.empty((num_vox+1000), dtype=torch.int64, device=self.device)\n",
    "        final_pred = torch.empty((num_vox+1000), dtype=torch.int64, device=self.device)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for it, batch in tqdm(enumerate(loader), desc=\"Validation:\"):\n",
    "      \n",
    "            samples_hv, labels = self.sample_to_encode(it, batch) # Only return the features that haven't been dropped\n",
    "            \n",
    "            for b in range(0, samples_hv.shape[0], self.point_per_iter):\n",
    "                end = min(b + self.point_per_iter, int(samples_hv.shape[0]))  # Ensure we don't exceed num_voxels[i]\n",
    "                samples_hv_here = samples_hv[b:end]\n",
    "                labels_here = labels[b:end]\n",
    "                #torch.cuda.synchronize(device=self.device)\n",
    "            \n",
    "                shape_sample = labels_here.shape[0]\n",
    "\n",
    "                #pred_hd = self.model(samples_hv, dot=True).argmax(1).data\n",
    "                sim = self.model(samples_hv_here, dot=True)\n",
    "                #torch.cuda.synchronize(device=self.device)\n",
    "\n",
    "                pred_hd = torch.argmax(sim, axis=1)\n",
    "                #torch.cuda.synchronize(device=self.device)\n",
    "\n",
    "                #print(\"Labels: \", labels.shape[0])\n",
    "                #print(start_idx, start_idx+shape_sample)\n",
    "                #print(shape_sample)\n",
    "\n",
    "                final_labels[start_idx:start_idx+shape_sample] = labels_here\n",
    "                final_pred[start_idx:start_idx+shape_sample] = pred_hd\n",
    "\n",
    "                start_idx += shape_sample\n",
    "\n",
    "            if it == self.max_samples:\n",
    "                break\n",
    "\n",
    "        final_labels = final_labels[:start_idx]\n",
    "        final_pred = final_pred[:start_idx]\n",
    "\n",
    "        print(\"================================\")\n",
    "\n",
    "        #print('pred_ts', pred_ts)\n",
    "        print('pred_hd', final_pred, \"\\tShape: \", final_pred.shape)\n",
    "        print('label', final_labels, \"\\tShape: \", final_labels.shape)\n",
    "        accuracy = miou(final_pred, final_labels)\n",
    "        avg_acc = torch.mean(accuracy)\n",
    "        print(f'accuracy: {accuracy}')\n",
    "        print(f'avg acc: {avg_acc}')\n",
    "\n",
    "        if args.wandb_run:\n",
    "            log_data = {f\"Training class_{i}_IoU\": c for i, c in enumerate(accuracy)}\n",
    "            log_data[\"Retraining epoch\"] = avg_acc\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        #cm = confusion_matrix(pred_hd, first_label, labels=torch.Tensor(range(0,15)))\n",
    "        #print(\"Confusion matrix \\n\")\n",
    "        #print(cm)\n",
    "\n",
    "        print(\"================================\")\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-stops', '--layers', nargs='+', type=int, help='how many layers deep', default=[24, 36])\n",
    "    parser.add_argument('--confidence', type=float, help=\"Confidence threshold\", default=1.0)\n",
    "    #parser.add_argument('-soa', '--soa', action=\"store_true\", default=False, help='Plot SOA')\n",
    "    parser.add_argument('-number_samples', '--number_samples', type=int, help='how many scans to train', default=500)\n",
    "    parser.add_argument(\n",
    "            \"--seed\", default=None, type=int, help=\"Seed for initializing training\"\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            \"--add_lr\", action=\"store_true\", default=False, help='Add lr to help class imbalance'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            \"--dataset\", choices=['nuscenes', 'semantic_kitti', 'tls'], default='nuscenes', help='Which dataset to train and test on?'\n",
    "        )\n",
    "    parser.add_argument(\"--wandb_run\", action=\"store_true\", default=False, help='Pass values to WandDB')\n",
    "    parser.add_argument(\"--device\", choices=['gpu', 'cpu'], default='gpu', help='Which device to use for training')\n",
    "\n",
    "    # HD arguments\n",
    "    parser.add_argument('--dim', type=int, help='Dimensionality of Hypervectors', default=10000)\n",
    "    parser.add_argument('--batch_points', type=int, help='Number of points to process per scan', default=20000)\n",
    "    #parser.add_argument('-val', '--val', action=\"store_true\", default=False, help='Train with validation for each scan')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e37efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarthiv02\u001b[0m (\u001b[33mdarth-iv-02\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.661 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Checkpoint loaded on cuda:0: /root/main/ScaLR/saved_models/ckpt_last_scalr.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loader: 100%|████████████████████████████████████████████████████████████████████| 282/282 [01:26<00:00,  3.26it/s]\n",
      "Validation loader: 100%|████████████████████████████████████████████████████████████████████| 61/61 [00:16<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_dict_args(args_dict):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Define arguments dynamically based on the dictionary keys\n",
    "    for key, value in args_dict.items():\n",
    "        if value is not None:\n",
    "            arg_type = type(value)\n",
    "            parser.add_argument(f'--{key}', type=arg_type, default=value)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Simulate command-line arguments\n",
    "    args_list = [f'--{key}={value}' for key, value in args_dict.items()]\n",
    "    \n",
    "    return parser.parse_args(args_list)\n",
    "\n",
    "og_args = {'confidence':1, 'number_samples': 500, 'add_lr': False, \n",
    "        \"dataset\":'nuscenes', \"wandb_run\":False, 'device':'gpu', 'dim':10000, 'batch_points':20000}\n",
    "\n",
    "args = parse_dict_args(og_args)\n",
    "\n",
    "args.seed = None\n",
    "args.layers = ['24', '36']\n",
    "\n",
    "# Set seed\n",
    "if args.seed is not None:\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(args.seed)\n",
    "\n",
    "DIMENSIONS = args.dim\n",
    "FEAT_SIZE = 768\n",
    "NUM_LEVELS = 8000\n",
    "BATCH_SIZE = 1  # for GPUs with enough memory we can process multiple images at ones\n",
    "\n",
    "wandb.login(key=\"9487c04b8eff0c16cac4e785f2b57c3a475767d3\")\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and args.device == 'gpu') else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n",
    "device_string = \"cuda:0\" if (torch.cuda.is_available() and args.device == 'gpu') else \"cpu\"\n",
    "\n",
    "# Modify the path for each of the folders\n",
    "\n",
    "if args.dataset == 'nuscenes':\n",
    "    path = '/root/main/dataset/nuscenes'\n",
    "elif args.dataset == 'semantic_kitti':\n",
    "    path = '/root/main/dataset/semantickitti'\n",
    "elif args.dataset == 'tls':\n",
    "    path = '/root/main/dataset/tls'\n",
    "\n",
    "\n",
    "# Get datatset\n",
    "DATASET = LIST_DATASETS.get(args.dataset)\n",
    "\n",
    "##### Process dataset #######\n",
    "\n",
    "if args.dataset == 'nuscenes':\n",
    "\n",
    "    kwargs = {\n",
    "        \"rootdir\": path,\n",
    "        \"input_feat\": [\"intensity\", \"xyz\", \"radius\"],\n",
    "        \"voxel_size\": 0.1,\n",
    "        \"num_neighbors\": 16,\n",
    "        \"dim_proj\": [2, 1, 0],\n",
    "        \"grids_shape\": [[256, 256], [256, 32], [256, 32]],\n",
    "        \"fov_xyz\": [[-64, -64, -8], [64, 64, 8]], # Check here\n",
    "    }\n",
    "\n",
    "    # Train dataset\n",
    "    dataset = DATASET(\n",
    "        phase=\"train\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    dataset_train = copy.deepcopy(dataset)\n",
    "    dataset_val = copy.deepcopy(dataset)\n",
    "    dataset_train.init_training()\n",
    "    dataset_val.init_val()\n",
    "\n",
    "    num_classes = 16\n",
    "\n",
    "    path_pretrained = '/root/main/ScaLR/saved_models/ckpt_last_scalr.pth'\n",
    "\n",
    "elif args.dataset == 'semantic_kitti':\n",
    "\n",
    "    kwargs = {\n",
    "        \"rootdir\": path,\n",
    "        \"input_feat\": [\"intensity\", \"xyz\", \"radius\"],\n",
    "        \"voxel_size\": 0.1,\n",
    "        \"num_neighbors\": 16,\n",
    "        \"dim_proj\": [2, 1, 0],\n",
    "        \"grids_shape\": [[256, 256], [256, 32], [256, 32]],\n",
    "        \"fov_xyz\": [[-64, -64, -8], [64, 64, 8]], # Check here\n",
    "    }\n",
    "\n",
    "    dataset_train = DATASET(\n",
    "        phase=\"specific_train\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = DATASET(\n",
    "        phase=\"val\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    num_classes = 19\n",
    "\n",
    "    path_pretrained = '/root/main/ScaLR/saved_models/ckpt_last_kitti.pth'\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Dataset Not identified\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=Collate(device=device),\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=Collate(device=device),\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "hd_model = HD_Model(FEAT_SIZE, DIMENSIONS, num_classes, path_pretrained, device=device, args=args)\n",
    "hd_model.set_loaders(train_loader=train_loader, val_loader=val_loader)\n",
    "\n",
    "\n",
    "####### SOA results ##########\n",
    "#print(\"SoA results\")\n",
    "\n",
    "#hd_model.feature_extractor.test(hd_model.val_loader, hd_model.num_vox_val+1000, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d79e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HyperLiDAR/wandb/run-20250318_042816-nuscenes_training_layers_['24', '36']_norm_dim_10000_OFA_early_exit</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/darth-iv-02/scalr_hd/runs/nuscenes_training_layers_%5B%2724%27%2C%20%2736%27%5D_norm_dim_10000_OFA_early_exit' target=\"_blank\">nuscenes_training_layers_['24', '36']_norm_dim_10000_OFA_early_exit</a></strong> to <a href='https://wandb.ai/darth-iv-02/scalr_hd' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/darth-iv-02/scalr_hd' target=\"_blank\">https://wandb.ai/darth-iv-02/scalr_hd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/darth-iv-02/scalr_hd/runs/nuscenes_training_layers_%5B%2724%27%2C%20%2736%27%5D_norm_dim_10000_OFA_early_exit' target=\"_blank\">https://wandb.ai/darth-iv-02/scalr_hd/runs/nuscenes_training_layers_%5B%2724%27%2C%20%2736%27%5D_norm_dim_10000_OFA_early_exit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Training\n",
      "\n",
      "Train First\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]/home/HyperLiDAR/models/waffleiron/helper_projection.py:27: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1550.)\n",
      "  residual.scatter_reduce_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4886, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.8081, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14603/1888630234.py:36: DeprecationWarning: torchhd.hard_quantize is deprecated, consider using torchhd.normalize instead.\n",
      "  return torchhd.hard_quantize(sample_hv)\n",
      "\r",
      "Training: 1it [00:09,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4867, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.6762, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 2it [00:13,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4663, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7630, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 3it [00:16,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4923, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7706, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 4it [00:20,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4262, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7439, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 5it [00:23,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4593, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7349, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 6it [00:25,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4986, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.6900, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 7it [00:29,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4501, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7637, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 8it [00:34,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4421, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.6897, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 9it [00:36,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.5085, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7487, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 10it [02:01, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4516, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7969, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 11it [02:04, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4708, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7536, device='cuda:0', dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 12it [02:07, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 cka loss:  tensor(0.4442, device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Layer 36 cka loss:  tensor(0.7666, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "hd_model.set_compensation({24: 'linear_weights_24_0.75.pth', 36: 'linear_weights_36_ep_0.75.pth'} )\n",
    "\n",
    "if args.wandb_run:\n",
    "    run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"scalr_hd\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"encoding\": \"Random Projection\",\n",
    "            \"hd_dim\": DIMENSIONS,\n",
    "            \"training_samples\":args.number_samples,\n",
    "        },\n",
    "        id=f\"{args.dataset}_training_layers_{args.layers}_norm_dim_{DIMENSIONS}_OFA_early_exit\",\n",
    "    )\n",
    "\n",
    "####### HD Pipeline ##########\n",
    "\n",
    "print(\"Initial Training\")\n",
    "hd_model.train()\n",
    "\n",
    "print(\"Testing\")\n",
    "hd_model.test_hd()\n",
    "\n",
    "print(\"Retraining\")\n",
    "hd_model.retrain(epochs=10)\n",
    "\n",
    "print(\"Testing\")\n",
    "hd_model.test_hd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e442a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
