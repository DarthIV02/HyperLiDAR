{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780291a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.scatter_reduce for 3D to 2D projection.\n",
      "Using torch.scatter_reduce for 3D to 2D projection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35737/3329173639.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('./saved_models/ckpt_last_scalr.pth', map_location=\"cuda:0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.647 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from models.waffleiron.segmenter import Segmenter\n",
    "import torch\n",
    "from datasets import LIST_DATASETS, Collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = Segmenter(\n",
    "    input_channels=5,\n",
    "    feat_channels=768,\n",
    "    depth=48,\n",
    "    grid_shape=[[256, 256], [256, 32], [256, 32]],\n",
    "    nb_class=16, # class for prediction\n",
    "    #drop_path_prob=config[\"waffleiron\"][\"drop_path\"],\n",
    "    layer_norm=True,\n",
    ")\n",
    "\n",
    "# Load pretrained model\n",
    "ckpt = torch.load('./saved_models/ckpt_last_scalr.pth', map_location=\"cuda:0\")\n",
    "ckpt = ckpt[\"net\"]\n",
    "\n",
    "new_ckpt = {}\n",
    "for k in ckpt.keys():\n",
    "    if k.startswith(\"module\"):\n",
    "        if k.startswith(\"module.classif.0\"):\n",
    "            continue\n",
    "        elif k.startswith(\"module.classif.1\"):\n",
    "            new_ckpt[\"classif\" + k[len(\"module.classif.1\") :]] = ckpt[k]\n",
    "        else:\n",
    "            new_ckpt[k[len(\"module.\") :]] = ckpt[k]\n",
    "    else:\n",
    "        new_ckpt[k] = ckpt[k]\n",
    "\n",
    "model.load_state_dict(new_ckpt)\n",
    "\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "model = model.cuda(\"cuda:0\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "kwargs = {\n",
    "        \"rootdir\": '/root/main/dataset/nuscenes',\n",
    "        \"input_feat\": [\"xyz\", \"intensity\", \"radius\"],\n",
    "        \"voxel_size\": 0.1,\n",
    "        \"num_neighbors\": 16,\n",
    "        \"dim_proj\": [2, 1, 0],\n",
    "        \"grids_shape\": [[256, 256], [256, 32], [256, 32]],\n",
    "        \"fov_xyz\": [[-64, -64, -8], [64, 64, 8]], # Check here\n",
    "    }\n",
    "\n",
    "# Get datatset\n",
    "DATASET = LIST_DATASETS.get(\"nuscenes\")\n",
    "if DATASET is None:\n",
    "    raise ValueError(f\"Dataset {args.dataset.lower()} not available.\")\n",
    "\n",
    "# Train dataset\n",
    "train_dataset = DATASET(\n",
    "    phase=\"val\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=Collate(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9c7b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m             out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mnet_inputs, stop)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m out, embed, tokens \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m], out[\u001b[38;5;241m1\u001b[39m], \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Voxels to points\u001b[39;00m\n\u001b[1;32m     29\u001b[0m token_upsample \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for stop in range(0,49,3):\n",
    "    for it, batch in enumerate(train_loader):\n",
    "        \n",
    "        if it == 15: # Only the first sample\n",
    "\n",
    "            # Network inputs\n",
    "            #print(batch[\"upsample\"])\n",
    "            feat = batch[\"feat\"].cuda(0, non_blocking=True)\n",
    "            labels = batch[\"labels_orig\"].cuda(0, non_blocking=True)\n",
    "            batch[\"upsample\"] = [\n",
    "                up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "            ]\n",
    "            cell_ind = batch[\"cell_ind\"].cuda(0, non_blocking=True)\n",
    "            occupied_cell = batch[\"occupied_cells\"].cuda(0, non_blocking=True)\n",
    "            neighbors_emb = batch[\"neighbors_emb\"].cuda(0, non_blocking=True)\n",
    "            net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(*net_inputs, stop)\n",
    "\n",
    "            break\n",
    "    \n",
    "    out, embed, tokens = out[0], out[1], out[2]\n",
    "    \n",
    "    # Voxels to points\n",
    "    token_upsample = []\n",
    "    temp = None\n",
    "    for id_b, closest_point in enumerate(batch[\"upsample\"]):\n",
    "        print(id_b)\n",
    "        print(closest_point)\n",
    "        temp = tokens[id_b, :, closest_point]\n",
    "        token_upsample.append(temp.T)\n",
    "    token_2 = torch.cat(token_upsample, dim=0)\n",
    "    \n",
    "    import math\n",
    "\n",
    "    mean_dist = torch.zeros((15,15))\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            if i != 8 and j != 8:\n",
    "                label_indices = torch.nonzero(labels == i).squeeze()\n",
    "                label_indices_2 = torch.nonzero(labels == j).squeeze()\n",
    "                distance = torch.cdist(token_2[label_indices, :], token_2[label_indices_2, :], p=2)\n",
    "                mean = torch.mean(distance)\n",
    "                if not math.isnan(mean):\n",
    "                    if i > 8:\n",
    "                        d_i = i-1\n",
    "                    else:\n",
    "                        d_i = i\n",
    "                    if j > 8:\n",
    "                        d_j = j - 1\n",
    "                    else:\n",
    "                        d_j = j\n",
    "                    mean_dist[d_i,d_j] = mean\n",
    "                    \n",
    "    # Option 1: Use matplotlib to visualize the matrix as a heatmap\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mean_dist.cpu(), cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()  # Add color bar to interpret the values\n",
    "    plt.title(f'Distance between classes layer {stop}')\n",
    "    plt.savefig(f'./results/nuscenes_features_2/distance_cmap_{stop}.png')  # Save the heatmap as an image\n",
    "    plt.show()  # Display the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbb1e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27e39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Note: this example requires the torchmetrics library: https://torchmetrics.readthedocs.io\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchhd\n",
    "from torchhd.models import Centroid\n",
    "from torchhd import embeddings\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "DIMENSIONS = 10000\n",
    "FEAT_SIZE = 768\n",
    "NUM_LEVELS = 1000\n",
    "BATCH_SIZE = 1  # for GPUs with enough memory we can process multiple images at ones\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, out_features, size, levels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.position = embeddings.Random(size, out_features)\n",
    "        self.value = embeddings.Level(levels, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample_hv = torchhd.bind(self.position.weight, self.value(x))\n",
    "        sample_hv = torchhd.multiset(sample_hv)\n",
    "        return torchhd.hard_quantize(sample_hv)\n",
    "\n",
    "\n",
    "encode = Encoder(DIMENSIONS, FEAT_SIZE, NUM_LEVELS)\n",
    "encode = encode.to(device)\n",
    "\n",
    "num_classes = 16\n",
    "model_hd = Centroid(DIMENSIONS, num_classes)\n",
    "model_hd = model_hd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = 0\n",
    "for it, batch in enumerate(train_loader):\n",
    "    if it == 0:\n",
    "        # Network inputs\n",
    "        #print(batch[\"upsample\"])\n",
    "        feat = batch[\"feat\"].cuda(0, non_blocking=True)\n",
    "        labels = batch[\"labels_orig\"].cuda(0, non_blocking=True)\n",
    "        batch[\"upsample\"] = [\n",
    "            up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "        ]\n",
    "        cell_ind = batch[\"cell_ind\"].cuda(0, non_blocking=True)\n",
    "        occupied_cell = batch[\"occupied_cells\"].cuda(0, non_blocking=True)\n",
    "        neighbors_emb = batch[\"neighbors_emb\"].cuda(0, non_blocking=True)\n",
    "        #net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(feat, cell_ind, occupied_cell, neighbors_emb, stop)\n",
    "            embed, tokens = out[0][0], out[1][0]\n",
    "            embed = embed.transpose(0, 1)\n",
    "            tokens = tokens.transpose(0, 1)\n",
    "            \n",
    "            print(\"Tokens\", tokens.shape)\n",
    "            \n",
    "            labels_v = [[] for i in range(embed.shape[0])]\n",
    "            for i, vox in enumerate(batch[\"upsample\"][0]):\n",
    "                labels_v[vox].append(labels[i])\n",
    "            labels_v_single = []\n",
    "            for labels_ in labels_v:\n",
    "                lab_tens = torch.tensor(labels_)\n",
    "                most_common_value = torch.bincount(lab_tens).argmax()\n",
    "                labels_v_single.append(most_common_value)\n",
    "                \n",
    "            #HD Training\n",
    "            for samples, labels in tqdm(zip(tokens,labels_v_single), desc=\"Training\"):\n",
    "                if labels != 255:\n",
    "                    samples = samples.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    samples_hv = encode(samples).reshape((1, DIMENSIONS))\n",
    "                    model_hd.add(samples_hv, labels)\n",
    "            \n",
    "\n",
    "            # Voxels to points\n",
    "            #token_upsample = []\n",
    "            #temp = None\n",
    "            #for id_b, closest_point in enumerate(batch[\"upsample\"]):\n",
    "            #    temp = tokens[id_b, :, closest_point]\n",
    "            #    token_upsample.append(temp.T)\n",
    "            #token_2 = torch.cat(token_upsample, dim=0)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ed6c7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 17483])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27f228c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  312,     1,     4,    81,     4,     0,   113,    16,     0,   494,\n",
      "        12172,   629,   817,  2602,  4873,  4034,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,  8536], device='cuda:0')\n",
      "17483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35039/4106614746.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.bincount(torch.tensor(labels)))\n"
     ]
    }
   ],
   "source": [
    "print(torch.bincount(torch.tensor(labels)))\n",
    "print(len(labels_v_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c96cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for samples, labels in tqdm(train_ld, desc=\"Training\"):\n",
    "        samples = samples.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        samples_hv = encode(samples)\n",
    "        model.add(samples_hv, labels)\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.normalize()\n",
    "\n",
    "    for samples, labels in tqdm(test_ld, desc=\"Testing\"):\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        samples_hv = encode(samples)\n",
    "        outputs = model(samples_hv, dot=True)\n",
    "        accuracy.update(outputs.cpu(), labels)\n",
    "\n",
    "print(f\"Testing accuracy of {(accuracy.compute().item() * 100):.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_2",
   "language": "python",
   "name": "env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
