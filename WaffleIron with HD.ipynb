{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780291a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.scatter_reduce for 3D to 2D projection.\n",
      "Using torch.scatter_reduce for 3D to 2D projection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36040/2189106627.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('./saved_models/ckpt_last_scalr.pth', map_location=\"cuda:0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.617 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "323\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.565 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from models.waffleiron.segmenter import Segmenter\n",
    "import torch\n",
    "from datasets import LIST_DATASETS, Collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = Segmenter(\n",
    "    input_channels=5,\n",
    "    feat_channels=768,\n",
    "    depth=48,\n",
    "    grid_shape=[[256, 256], [256, 32], [256, 32]],\n",
    "    nb_class=16, # class for prediction\n",
    "    #drop_path_prob=config[\"waffleiron\"][\"drop_path\"],\n",
    "    layer_norm=True,\n",
    ")\n",
    "\n",
    "# Load pretrained model\n",
    "ckpt = torch.load('./saved_models/ckpt_last_scalr.pth', map_location=\"cuda:0\")\n",
    "ckpt = ckpt[\"net\"]\n",
    "\n",
    "new_ckpt = {}\n",
    "for k in ckpt.keys():\n",
    "    if k.startswith(\"module\"):\n",
    "        if k.startswith(\"module.classif.0\"):\n",
    "            continue\n",
    "        elif k.startswith(\"module.classif.1\"):\n",
    "            new_ckpt[\"classif\" + k[len(\"module.classif.1\") :]] = ckpt[k]\n",
    "        else:\n",
    "            new_ckpt[k[len(\"module.\") :]] = ckpt[k]\n",
    "    else:\n",
    "        new_ckpt[k] = ckpt[k]\n",
    "\n",
    "model.load_state_dict(new_ckpt)\n",
    "\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "model = model.cuda(\"cuda:0\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "kwargs = {\n",
    "        \"rootdir\": '/root/main/dataset/nuscenes',\n",
    "        \"input_feat\": [\"xyz\", \"intensity\", \"radius\"],\n",
    "        \"voxel_size\": 0.1,\n",
    "        \"num_neighbors\": 16,\n",
    "        \"dim_proj\": [2, 1, 0],\n",
    "        \"grids_shape\": [[256, 256], [256, 32], [256, 32]],\n",
    "        \"fov_xyz\": [[-64, -64, -8], [64, 64, 8]], # Check here\n",
    "    }\n",
    "\n",
    "# Get datatset\n",
    "DATASET = LIST_DATASETS.get(\"nuscenes\")\n",
    "if DATASET is None:\n",
    "    raise ValueError(f\"Dataset {args.dataset.lower()} not available.\")\n",
    "\n",
    "# Train dataset\n",
    "train_dataset = DATASET(\n",
    "    phase=\"train\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "test_dataset = DATASET(\n",
    "    phase=\"val\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=Collate(),\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=Collate(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbb1e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da0d689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27e39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Note: this example requires the torchmetrics library: https://torchmetrics.readthedocs.io\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchhd\n",
    "from torchhd.models import Centroid\n",
    "from torchhd import embeddings\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "DIMENSIONS = 10000\n",
    "FEAT_SIZE = 768\n",
    "NUM_LEVELS = 1000\n",
    "BATCH_SIZE = 1  # for GPUs with enough memory we can process multiple images at ones\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, out_features, size, levels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.position = embeddings.Random(size, out_features)\n",
    "        self.value = embeddings.Level(levels, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample_hv = torchhd.bind(self.position.weight, self.value(x))\n",
    "        sample_hv = torchhd.multiset(sample_hv)\n",
    "        return torchhd.hard_quantize(sample_hv)\n",
    "\n",
    "\n",
    "encode = Encoder(DIMENSIONS, FEAT_SIZE, NUM_LEVELS)\n",
    "encode = encode.to(device)\n",
    "\n",
    "num_classes = 16\n",
    "model_hd = Centroid(DIMENSIONS, num_classes)\n",
    "model_hd = model_hd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1faa8f83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]/tmp/ipykernel_36040/3614351508.py:35: DeprecationWarning: torchhd.hard_quantize is deprecated, consider using torchhd.normalize instead.\n",
      "  return torchhd.hard_quantize(sample_hv)\n",
      "Training: 17483it [00:11, 1475.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]/tmp/ipykernel_36040/1755722160.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = torch.tensor(outputs.argmax().data, device='cuda:0').reshape((1))\n",
      "Testing: 17777it [00:16, 1050.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1041.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1049.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 15.405%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 17603it [00:11, 1474.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 15491it [00:10, 1481.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:17, 1042.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1042.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1039.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 17.130%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 15241it [00:10, 1490.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:16, 1047.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1041.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1042.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 19.313%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 16127it [00:10, 1472.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:16, 1048.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1035.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1050.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 10.157%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 16873it [00:11, 1491.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:16, 1048.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1056.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:15, 1056.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 14.383%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 17182it [00:11, 1479.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:17, 1034.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1042.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1036.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 14.028%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 17020it [00:11, 1469.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:17, 1039.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1043.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1048.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 14.124%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 16962it [00:11, 1477.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:16, 1047.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1047.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1050.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 14.540%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 16933it [00:11, 1489.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:16, 1054.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1047.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1050.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 14.277%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 16561it [00:11, 1490.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17777it [00:16, 1054.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16314it [00:15, 1044.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16879it [00:16, 1045.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 12.488%\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 6161it [00:04, 1462.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m         samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 36\u001b[0m         samples_hv \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, DIMENSIONS))\n\u001b[1;32m     37\u001b[0m         model_hd\u001b[38;5;241m.\u001b[39madd(samples_hv, labels)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Voxels to points\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#token_upsample = []\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#temp = None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#    token_upsample.append(temp.T)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#token_2 = torch.cat(token_upsample, dim=0)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_4/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_4/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     sample_hv \u001b[38;5;241m=\u001b[39m \u001b[43mtorchhd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     sample_hv \u001b[38;5;241m=\u001b[39m torchhd\u001b[38;5;241m.\u001b[39mmultiset(sample_hv)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torchhd\u001b[38;5;241m.\u001b[39mhard_quantize(sample_hv)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_4/lib/python3.8/site-packages/torchhd/functional.py:669\u001b[0m, in \u001b[0;36mbind\u001b[0;34m(input, other)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ensure_vsa_tensor(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    668\u001b[0m other \u001b[38;5;241m=\u001b[39m ensure_vsa_tensor(other)\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_4/lib/python3.8/site-packages/torchhd/tensors/map.py:250\u001b[0m, in \u001b[0;36mMAPTensor.bind\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAPTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAPTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Bind the hypervector with other using element-wise multiplication.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    This produces a hypervector dissimilar to both.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_4/lib/python3.8/site-packages/torch/_tensor.py:1437\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1437\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1439\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop = 0\n",
    "for it, batch in enumerate(train_loader):\n",
    "    \n",
    "    # Network inputs\n",
    "    #print(batch[\"upsample\"])\n",
    "    feat = batch[\"feat\"].cuda(0, non_blocking=True)\n",
    "    labels = batch[\"labels_orig\"].cuda(0, non_blocking=True)\n",
    "    batch[\"upsample\"] = [\n",
    "        up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "    ]\n",
    "    cell_ind = batch[\"cell_ind\"].cuda(0, non_blocking=True)\n",
    "    occupied_cell = batch[\"occupied_cells\"].cuda(0, non_blocking=True)\n",
    "    neighbors_emb = batch[\"neighbors_emb\"].cuda(0, non_blocking=True)\n",
    "    #net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(feat, cell_ind, occupied_cell, neighbors_emb, stop)\n",
    "        embed, tokens = out[0][0], out[1][0]\n",
    "        embed = embed.transpose(0, 1)\n",
    "        tokens = tokens.transpose(0, 1)\n",
    "\n",
    "        labels_v = [[] for i in range(embed.shape[0])]\n",
    "        for i, vox in enumerate(batch[\"upsample\"][0]):\n",
    "            labels_v[vox].append(labels[i])\n",
    "        labels_v_single = []\n",
    "        for labels_ in labels_v:\n",
    "            lab_tens = torch.tensor(labels_)\n",
    "            most_common_value = torch.bincount(lab_tens).argmax()\n",
    "            labels_v_single.append(most_common_value)\n",
    "\n",
    "        #HD Training\n",
    "        for samples, labels in tqdm(zip(tokens,labels_v_single), desc=\"Training\"):\n",
    "            if labels != 255:\n",
    "                samples = samples.to(device)\n",
    "                labels = labels.to(device)\n",
    "                samples_hv = encode(samples).reshape((1, DIMENSIONS))\n",
    "                model_hd.add(samples_hv, labels)\n",
    "\n",
    "\n",
    "        # Voxels to points\n",
    "        #token_upsample = []\n",
    "        #temp = None\n",
    "        #for id_b, closest_point in enumerate(batch[\"upsample\"]):\n",
    "        #    temp = tokens[id_b, :, closest_point]\n",
    "        #    token_upsample.append(temp.T)\n",
    "        #token_2 = torch.cat(token_upsample, dim=0)\n",
    "    \n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c96cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "    \n",
    "    model_hd.normalize()\n",
    "    \n",
    "    stop = 0\n",
    "    for it, batch in enumerate(test_loader):\n",
    "        if it < 3:\n",
    "            # Network inputs\n",
    "            #print(batch[\"upsample\"])\n",
    "            feat = batch[\"feat\"].cuda(0, non_blocking=True)\n",
    "            labels = batch[\"labels_orig\"].cuda(0, non_blocking=True)\n",
    "            batch[\"upsample\"] = [\n",
    "                up.cuda(0, non_blocking=True) for up in batch[\"upsample\"]\n",
    "            ]\n",
    "            cell_ind = batch[\"cell_ind\"].cuda(0, non_blocking=True)\n",
    "            occupied_cell = batch[\"occupied_cells\"].cuda(0, non_blocking=True)\n",
    "            neighbors_emb = batch[\"neighbors_emb\"].cuda(0, non_blocking=True)\n",
    "            #net_inputs = (feat, cell_ind, occupied_cell, neighbors_emb)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(feat, cell_ind, occupied_cell, neighbors_emb, stop)\n",
    "                embed, tokens = out[0][0], out[1][0]\n",
    "                embed = embed.transpose(0, 1)\n",
    "                tokens = tokens.transpose(0, 1)\n",
    "\n",
    "                labels_v = [[] for i in range(embed.shape[0])]\n",
    "                for i, vox in enumerate(batch[\"upsample\"][0]):\n",
    "                    labels_v[vox].append(labels[i])\n",
    "                labels_v_single = []\n",
    "                for labels_ in labels_v:\n",
    "                    lab_tens = torch.tensor(labels_)\n",
    "                    most_common_value = torch.bincount(lab_tens).argmax()\n",
    "                    labels_v_single.append(most_common_value)\n",
    "\n",
    "                #HD Testing\n",
    "                for samples, l in tqdm(zip(tokens,labels_v_single), desc=\"Testing\"):\n",
    "                    samples = samples.to(device)\n",
    "\n",
    "                    samples_hv = encode(samples).reshape((1, DIMENSIONS))\n",
    "                    outputs = model_hd(samples_hv, dot=True)\n",
    "                    outputs = torch.tensor(outputs.argmax().data, device='cuda:0').reshape((1))\n",
    "                    l = torch.tensor([l])\n",
    "                    accuracy.update(outputs.cpu(), l)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"Testing accuracy of {(accuracy.compute().item() * 100):.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_2",
   "language": "python",
   "name": "env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
